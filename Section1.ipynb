{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "# **１章　ニューラルネットワークの復習**\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "本書は、前作『ゼロから作るDeep Learning』の続編です。  \n",
    "前作同様、ライブラリやフレームワークなどの既製品は使わずに、「ゼロから作る」ことを重視します。\n",
    "\n",
    "本章では、ニューラルネットワークの復習を行います。つまり、前作のダイジェスト版に相当します。  \n",
    "本作では効率性を重視して、前作での実装ルールを一部変更した点があります（メソッド名やパラメータ  \n",
    "の持ち方など）。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "## **1.1 数学とPython の復習**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "テーマは、ニューラルネットワークの計算に必要な「**ベクトル**」や「**行列**」  です。  \n",
    "合わせて、Python によるコード ――特にNumPy を使ったコード―― も示します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.1.1 ベクトルと行列**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ここでは、ニューラルネットワークを扱う際にいたるところで登場する、  \n",
    "「ベクトル」や「行列」（または「テンソル」）について簡単に整理します。  \n",
    "\n",
    "**ベクトル**とは、大きさと向きを持つ量で、数が一列に並んだ集まりとして表現できます。  \n",
    "Python による実装では１次元の配列として扱うことができます。  \n",
    "**行列**は、２次元状（長方形状）に並んだ数の集まりです。\n",
    "\n",
    "ベクトルと行列の例を示すと図1-1 のようになります。  \n",
    "\n",
    "![代替テキスト](../images/Section1/図1-1_ベクトルと行列の例.png)\n",
    "\n",
    "図1-1 で示すように、ベクトルは１次元の配列、行列は２次元の配列で表現できます。\n",
    "\n",
    "また行列では、横方向の並びを行（row）と言い、縦方向の並びを列（column）と言います。  \n",
    "そのため、図1-1 の行列は「３行２列の行列」と呼び、また「３×２の行列」と表記します。  \n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_1-1.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ベクトルの表現方法には2種類あります。  \n",
    "図1-2 に示すように、ひとつは縦方向の並びとして表す**列ベクトル**、もうひとつは横方向の並びとして表す**行ベクトル**です。  \n",
    "\n",
    "![代替テキスト](../images/Section1/図1-2_ベクトルの表し方.png)\n",
    "\n",
    "数学やディープラーニングなどの多くの分野では、ベクトルは「列ベクトル」として扱うのが一般的です。  \n",
    "ただし、本書では実装面の親和性を考慮して、ベクトルは「行ベクトル」として扱います。  \n",
    "また、ベクトルや行列を数式で書く際には、単一の要素（スカラ）とは区別するため、太字の記号（$\\bold{x}$ や $\\bold{W}$ など）で表します。  \n",
    "\n",
    "それでは、Python の対話モードを使って、ベクトルや行列を生成してみましょう。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x のクラス: <class 'numpy.ndarray'>\n",
      "x の形状　: (3,)\n",
      "x の次元数: 1\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "\n",
    "print(\"x のクラス:\", x.__class__) \n",
    "print(\"x の形状　:\", x.shape)\n",
    "print(\"x の次元数:\", x.ndim)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W の形状　:  (2, 3)\n",
      "W の次元数:  2\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"W の形状　: \", W.shape)\n",
    "print(\"W の次元数: \", W.ndim)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ベクトルや行列は <span style=\"font-family: consolas;\">np.array()</span> メソッドで生成することができます。  \n",
    "このメソッドによって、NumPy の多次元配列用のクラスである <span style=\"font-family: consolas;\">np.ndarray</span> （N-dimensional array: N次元配列）クラスが生成されます。  \n",
    "\n",
    "上の例では、<span style=\"font-family: consolas;\">np.ndarray</span> クラスのインスタンス変数である <span style=\"font-family: consolas;\">shape</span> と <span style=\"font-family: consolas;\">ndim</span> を利用しています。  \n",
    "<span style=\"font-family: consolas;\">shape</span> は多次元配列の形状を、<span style=\"font-family: consolas;\">ndim</span> は次元数を表します。  \n",
    "<span style=\"font-family: consolas;\">**x**</span> は１次元の配列であり、要素数は３のベクトルです。<span style=\"font-family: consolas;\">**W**</span> は２次元の配列であり、２×３の行列です。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.1.2 行列の要素ごとの演算**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "数の集まりを「ベクトル」や「行列」としてまとめることができました。  \n",
    "それでは、それらを使って簡単な計算を行ってみましょう。  \n",
    "ここでは初めに「要素ごとの演算」について見ていきます。  \n",
    "<!-- ちなみに、「要素ごとの」とは英語で element-wise と言います。 -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  5]\n",
      " [ 7  9 11]]\n",
      "\n",
      "[[ 0  2  6]\n",
      " [12 20 30]]\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "X = np.array([[0, 1, 2], [3, 4, 5]])\n",
    "\n",
    "print(W + X)\n",
    "print()\n",
    "print(W * X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ここでは NumPy の多次元配列に対して、＋（足し算）や ＊（掛け算）などの四則演算を行っています。  \n",
    "このとき、多次元配列中の要素ごとで ――各要素で独立して―― 演算が行われます。  \n",
    "これが NumPy 配列の「要素ごとの演算」です。  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.1.3 ブロードキャスト**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "NumPy の多次元配列では、形状の異なる配列どうしの演算も可能です。  \n",
    "たとえば、次のような計算です。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 20],\n",
       "       [30, 40]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "A * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "この計算では、２×２の行列 <span style=\"font-family: consolas;\">**A**</span> に対して、10というスカラ値の乗算を行っています。\n",
    "\n",
    "このとき図1-3 で示すように、10というスカラ値が２×２の行列に拡張されて、その後で要素ごとの演算が行われます。  \n",
    "この賢い機能は、<span style=\"color: red;\">ブロードキャスト</span>（broadcast）と呼ばれます。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-3_ブロードキャストの例1.png)  \n",
    "\n",
    "ブロードキャストの別の例として、次の計算を見てみましょう。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 40],\n",
       "       [30, 80]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([10, 20])\n",
    "A * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "この計算では、図1-4 のように、１次元配列である <span style=\"font-family: consolas;\">**b**</span> が２次元配列 <span style=\"font-family: consolas;\">**A**</span> と同じ形状になるように\"賢く\"拡張されます。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-4_ブロードキャストの例2.png)  \n",
    "\n",
    "このように、NumPy ではブロードキャストという機能があるため、形状の異なる配列どうしの演算をスマートに行うことができます。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.1.4 ベクトルの内積と行列の積**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "続いて、「ベクトルの内積」と「行列の積」について見ていきます。  \n",
    "ベクトルの内積は、数式で次のように表されます。\n",
    "\n",
    "$ \\qquad\\qquad\\qquad \\bold{x}\\sdot\\bold{y}=x_1y_1 + x_2y_2 + \\dots + x_ny_n \\qquad\\qquad (1.1) $\n",
    "\n",
    "ここでは２つのベクトル $\\bold{x} = (x_1, \\dots , x_n)$ と $\\bold{y} = (y_1, \\dots , y_n)$ があると仮定します。  \n",
    "このとき式$(1.1)$ が示すように、ベクトルの内積は、２つのベクトル間の対応する要素の  \n",
    "積を足し合わせたものになります。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_1-2.png) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "続いて「行列の積」についてです。行列の積は、図1-5 の手順に従って計算します。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-5_行列の積の計算方法.png) \n",
    "\n",
    "図1-5 のとおり、行列の積は「左の行列の行ベクトル」と「右の行列の列ベクトル」の内積によって計算されます。  \n",
    "このとき、その計算結果は新しい行列の対応する要素に格納されます。  \n",
    "たとえば、$\\bold{A}$ の１行目と $\\bold{B}$ の１列目の結果は１行１列目の要素へ、$\\bold{A}$ の２行目と$\\bold{B}$  の１列目の結果は２行１列目の要素へ...といったようになります。  \n",
    "ベクトルの内積と行列の積を Python で実装すると次のようになります。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ベクトルの内積\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "np.dot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 行列の積\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ここで示したように、ベクトルの内積も、行列の積も <span style=\"font-family: Consolas\">np.dot()</span> で計算できます。  \n",
    "<span style=\"font-family: Consolas\">np.dot(x, y)</span> の引数がともに１次元配列の場合はベクトルの内積を計算し、引数が２次元配列の場合は行列の積を計算します。\n",
    "![代替テキスト](..\\images/Section1/Hint1_1-3.png) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.1.5 行列の形状チェック**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "行列やベクトルを使った計算においては、その「形状」に注意を払うことが重要です。  \n",
    "ここでは「行列の積」に関して、形状に注目して再度見ていきたいと思います。  \n",
    "行列の積を計算するときは、図1-6 で示す「形状チェック」が重要になります。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-6_形状チェック.png) \n",
    "\n",
    "図1-6 では、３×２の行列$\\bold{A}$ と２×４の行列$\\bold{B}$ の積によって、３×４の行列$\\bold{C}$ が生成されています。  \n",
    "このとき、図が示すように、$\\bold{A}$ と $\\bold{B}$ の対応する次元の要素数を一致させる必要があります。  \n",
    "そして、結果となる行列$\\bold{C}$ は、$\\bold{A}$の行数と $\\bold{B}$の列数から構成されるようにします。これが行列の「形状チェック」です。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_1-4.png) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "## **1.2 ニューラルネットワークの推論**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ここから、ニューラルネットワークの復習です。  \n",
    "ニューラルネットワークで行う処理は、「学習」と「推論」の２つのフェーズに分けられます。  \n",
    "ここではニューラルネットワークの「推論」だけにフォーカスし、次節で「学習」について見ていきます。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.2.1 ニューラルネットワークの推論の全体図**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ニューラルネットワークは、簡単に言ってしまえば、単なる「関数」です。  \n",
    "関数とは、\"何かを入力したら何かが出力される変換器\" ですが、ニューラルネットワークも入力を出力へと変換します。  \n",
    "\n",
    "ここでは例として、２次元のデータを入力して、３次元のデータを出力する関数を考えます。  \n",
    "これをニューラルネットワークで実現するには、入力層にニューロンを２つ、出力層にニューロンを３つそれぞれ用意します。  \n",
    "そして隠れ層（中間層）にもいくつかニューロンを配置します。ここでは隠れ層に４つのニューロンを置くことにしましょう。  \n",
    "\n",
    "そうすると、私たちのニューラルネットワークは図1-7 のように書けます。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-7_ニューラルネットワークの例.png)\n",
    "\n",
    "図1-7 では、ニューロンを ○ で表し、そのつながりを矢印で表しています。  \n",
    "このとき、矢印に**重み**が存在し、その重みとニューロンの値がそれぞれ乗算され、その和が次のニューロンへの入力となります。  \n",
    "※正確には、それに活性化関数を適用した値が次のニューロンへの入力となる。\n",
    "\n",
    "またこのとき、前層のニューロンの値には影響を受けない「定数」も加算されます。この定数は **バイアス** と呼ばれます。  \n",
    "なお、図1-7 のニューラルネットワークは、隣接するニューロン間のすべてに結びつきがあります。これを **全結合層** と呼びます。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_1-5.png) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "それでは、図1-7 のニューラルネットワークが行う計算を数式で表します。\n",
    "\n",
    "ここでは入力層のデータを $(x_1, x_2)$ で表し、重みを $w_{11}$ と $w_{21}$、バイアスを $b_1$ と表します。  \n",
    "そうすると、図1-7 で示す隠れ層のうちの１番上のニューロンは、次のように計算されます。\n",
    "\n",
    "$ \\quad $  \n",
    "$ \\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\n",
    "    h_1 = x_1w_{11} + x_2w_{21} + b_1\n",
    "\\qquad (1.2)$\n",
    "\n",
    "$\\quad$  \n",
    "式$(1.2)$ のように、隠れ層のニューロンは重み付き和によって計算されます。  \n",
    "あとは、重みとバイアスの値を変えながら、式$(1.2)$ の計算をニューロンの数だけ繰り返し行うことで、隠れ層のすべてのニューロンの値を求めることができます。  \n",
    "\n",
    "ここで重要なことは、重みとバイアスの添字の規則ではなく、それが「重み付き和で計算される」ことと「行列の積でまとめて計算できる」ということです。  \n",
    "実際、全結合層による変換は、行列の積として、次のようにまとめて書くことができます。\n",
    "\n",
    "$ \\quad $  \n",
    "$ \\displaystyle\\qquad\n",
    "    (h_1, h_2, h_3, h_4) = (x1, x2)\n",
    "    \\begin{pmatrix}\n",
    "        w_{11} & w_{12} & w_{13} & w_{14}\\\\\n",
    "        w_{21} & w_{22} & w_{23} & w_{24}\n",
    "    \\end{pmatrix}\n",
    "     + (b_1, b_2, b_3, b_4)\n",
    "\\qquad (1.3) $\n",
    "\n",
    "$\\quad$  \n",
    "ここで、隠れ層のニューロンは $(h_1, h_2, h_3, h_4)$ としてまとめており、これは１×４の行列とみなすことができます。  \n",
    "また入力は $(x_1, x_2)$ であり、これは１×２の行列です。そして重みは２×４の行列、バイアスは１×４の行列に対応します。  \n",
    "そうすると、式$(1.3)$ は次のように簡略化して書くことができます。\n",
    "\n",
    "$ \\quad $  \n",
    "$ \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\n",
    "    \\bold{h} = \\bold{xW} + \\bold{b} \n",
    "\\qquad (1.4)$\n",
    "\n",
    "$\\quad$  \n",
    "入力を $\\bold{x}$、隠れ層のニューロンを $\\bold{h}$、重みを $\\bold{W}$、バイアスを $\\bold{b}$ で表しています。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "このとき、式$(1.4)$ の行列の形状に注目すると、次のように変換されることが分かります。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-8_形状チェック2.png) \n",
    "\n",
    "図1-8 に示すとおり、行列の積では、対応する次元の要素数を一致させます。  \n",
    "このように行列の形状を見ることで、それが正しい変換であることが確認できます。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_2-1.png) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "これで全結合層による変換を、行列としてまとめて計算することができました\n",
    " \n",
    "しかし、ここで行った変換はひとつのサンプルデータ（入力データ）だけを対象にしています。  \n",
    "ニューラルネットワークの分野では、複数のサンプルデータ（ミニバッチ）に対して、一斉に推論や学習を行います。  \n",
    "それを行うには、行列 $\\bold{x}$ の各行に個別のサンプルデータを格納します。  \n",
    "たとえば $N$ 個のサンプルデータをミニバッチとしてまとめて処理するとすれば、行列の形状に注目すると、次のように推移します。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-9_形状チェック3.png) \n",
    "\n",
    "図1-9 に示すように、形状チェックによって、各バッチが正しく変換されていることが分かります。  \n",
    "このとき、$N$ 個のサンプルデータがまとめて全結合層によって変換されており、隠れ層には$N$ 個分のニューロンがまとめて計算されています。  \n",
    "それでは、ミニバッチ版の全結合層による変換を Python で書いてみましょう。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(2, 4)  # 重み\n",
    "b1 = np.random.randn(4)     # バイアス\n",
    "x = np.random.randn(10, 2)  # 入力\n",
    "h = np.dot(x, W1) + b1      # 隠れ層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "(10, 2)\n",
      "[[ 0.62964936 -0.23540814]\n",
      " [-1.54783264  0.36316717]\n",
      " [-0.52929743  0.24543429]\n",
      " [-0.15850801 -0.67084833]\n",
      " [-0.14971846 -1.28941075]\n",
      " [ 0.6978794   1.09996458]\n",
      " [ 0.31659023  1.12895748]\n",
      " [ 0.55230081 -1.20663932]\n",
      " [-1.24205291  0.58852578]\n",
      " [-0.32005728 -0.22796009]]\n",
      "h\n",
      "(10, 4)\n",
      "[[ 0.47223223  3.56008703  0.41787645  0.14691142]\n",
      " [ 4.69166033  0.19605465 -1.30048621 -2.0809834 ]\n",
      " [ 2.77220856  1.66935636 -0.38954175 -1.06075577]\n",
      " [ 1.78157802  2.74535812 -0.63479552 -0.57152381]\n",
      " [ 1.55864977  3.13963307 -1.03480958 -0.47941144]\n",
      " [ 0.79252529  2.82882681  1.36609098  0.03404654]\n",
      " [ 1.50603362  2.28661328  1.01511817 -0.34584179]\n",
      " [ 0.29045534  4.05381799 -0.29868246  0.20166144]\n",
      " [ 4.2025306   0.47728217 -0.85481802 -1.80986503]\n",
      " [ 2.22778003  2.24957077 -0.49909588 -0.79056889]]\n"
     ]
    }
   ],
   "source": [
    "print(\"x\")\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(\"h\")\n",
    "print(h.shape)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "さて、全結合層による変換は「線形」な変換です。これに「非線形」な効果を与えるのが **活性化関数** です。  \n",
    "非線形な活性化関数を用いることで、ニューラルネットワークの表現力を増すことができるのです。  \n",
    "活性化関数にはさまざまありますが、ここでは式$(1.5)$ で表される **シグモイド関数** を使います。\n",
    "\n",
    "$ \\quad $  \n",
    "$ \\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\n",
    "    \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n",
    "\\qquad (1.5) $\n",
    "\n",
    "$ \\quad $  \n",
    "シグモイド関数は、図1-10 で示されるようなS字カーブの関数です。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-10_シグモイド関数のグラフ.png) \n",
    "\n",
    "シグモイド関数は入力として任意の実数を受け取り、０から１の間の実数を出力します。  \n",
    "早速このシグモイド関数を Python で実装しましょう。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "これは式$(1.5)$ をそのまま実装したものです。  \n",
    "それでは、このシグモイド関数 <span style='font-family: Consolas'>sigmoid()</span> を使って、先ほどの隠れ層のニューロン <span style='font-family: Consolas'>**h**</span> を変換します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sigmoid(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61591196, 0.97234992, 0.60297499, 0.53666194],\n",
       "       [0.99091191, 0.54885727, 0.2140832 , 0.11095892],\n",
       "       [0.94115542, 0.84148999, 0.40382762, 0.25716505],\n",
       "       [0.85589161, 0.93965066, 0.34642396, 0.36088529],\n",
       "       [0.82615952, 0.95849829, 0.26215273, 0.38239111],\n",
       "       [0.6883733 , 0.94421384, 0.79674786, 0.50851081],\n",
       "       [0.81847265, 0.90776227, 0.73402059, 0.41439114],\n",
       "       [0.5721076 , 0.98294011, 0.4258796 , 0.5502452 ],\n",
       "       [0.98526276, 0.61710589, 0.29842315, 0.14065444],\n",
       "       [0.90271658, 0.9046135 , 0.37775316, 0.31204653]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "これで シグモイド関数 によって、非線形な変換ができました。  \n",
    "続いて、この活性化関数の出力 <span style=\"font-family: Consolas\">**a**</span>（アクティベーションと呼ぶ。）を、また別の全結合層によって変換します。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-7_ニューラルネットワークの例.png) \n",
    "\n",
    "ここでは、隠れ層のニューロンは４つ、出力層のニューロンは３つなので、全結合層に使われる重みの行列は、４×３の形状に設定しなければなりません。  \n",
    "\n",
    "$\\qquad\\qquad\\qquad\\quad \\bold{h} \\qquad\\qquad \\bold{W} \\qquad = \\qquad\\quad\\bold{s} $\n",
    "\n",
    "$\\qquad\\qquad\\qquad N×4 \\qquad 4×3 \\qquad = \\qquad N×3 $\n",
    "\n",
    "これで出力層のニューロンを得ることができます。以上がニューラルネットワークの推論です。\n",
    "\n",
    "それでは、これまでの話をまとめて Python で書いてみます。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.random.randn(10, 2)\n",
    "W1 = np.random.randn(2, 4)  # 全結合層１の重み\n",
    "b1 = np.random.randn(4)     # 全結合層１のバイアス\n",
    "W2 = np.random.randn(4, 3)  # 全結合層２の重み\n",
    "b2 = np.random.randn(3)     # 全結合層２のバイアス\n",
    "\n",
    "h = np.dot(x, W1) + b1      # 隠れ層の入力\n",
    "a = sigmoid(h)              # 隠れ層の出力\n",
    "s = np.dot(a, W2) + b2      # 出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ここでは <span style=\"font-family: Consolas\">x</span> の形状は(10, 2)です。  \n",
    "これは２次元のデータが10個、ミニバッチとしてまとめられていることを意味します。\n",
    "\n",
    "そして最終的に出力される <span style=\"font-family: Consolas\">s</span> の形状は(10, 3)になります。  \n",
    "繰り返しになりますが、これは10個のデータがまとめて処理され、各データが３次元データに変換されたことを意味します。\n",
    "\n",
    "さて、上のニューラルネットワークは３次元のデータを出力します。  \n",
    "そのため、各次元の値を用いることで、３クラス分類を行うことができます。  \n",
    "その場合、出力された３次元ベクトルの各次元は各クラスの「スコア」に対応します（ひとつ目のニューロンがひとつ目のクラス、２つ目のニューロンが２つ目のクラス……）。  \n",
    "実際に分類を行うには、出力層のニューロンの値が一番大きい値を探し、そのニューロンに対応するクラスを結果とします。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_2-2.png)\n",
    "\n",
    "以上が、ニューラルネットワークの推論処理の実装になります。  \n",
    "続いて、ここで行った処理を「レイヤ」として Python のクラスを使って実装していきます。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.2.2 レイヤとしてのクラス化と順伝播の実装**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "それでは、ニューラルネットワークで行う処理を「レイヤ」として実装しましょう。  \n",
    "ここでは、全結合層による変換を Affine レイヤ、シグモイド関数による変換を Sigmoid レイヤとして実装します。  \n",
    "なお、全結合層による変換は幾何学の分野におけるアフィン変換に相当するため、Affine レイヤと名付けます。  \n",
    "また各レイヤは Python のクラスとして実装し、メインとなる変換を <span style=\"font-family: Consolas\">forward()</span> というメソッド名で実装することにします。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_2-3.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ニューラルネットワークには、さまざまなレイヤが登場します。私たちはそれを Python の**クラス**として実装します。  \n",
    "そのようなモジュール化を行うことで、レゴブロックを組み合わせるように、ネットワークを構築することができます。  \n",
    "本書では、そのようなレイヤを実装するにあたり、次の「実装ルール」を設けることにします。\n",
    "\n",
    "- すべてのレイヤは、メソッドとして <span style=\"font-family: Consolas\">forward()</span> と <span style=\"font-family: Consolas\">backward()</span> を持つ\n",
    "- すべてのレイヤは、インスタンス変数として <span style=\"font-family: Consolas\">params</span> と <span style=\"font-family: Consolas\">grads</span> を持つ\n",
    "\n",
    "この実装ルールについて簡単に説明します。  \n",
    "まずは <span style=\"font-family: Consolas\">forward()</span>メソッド と <span style=\"font-family: Consolas\">backward()</span>メソッド ですが、これはそれぞれが**順伝播**と**逆伝播**に対応します。  \n",
    "また、<span style=\"font-family: Consolas\">params</span> は重みやバイアスなどの**パラメータ**をリストとして保持します（パラメータは複数ある可能性があるのでリストとします）。  \n",
    "そして、<span style=\"font-family: Consolas\">grads</span> は、<span style=\"font-family: Consolas\">params</span> のパラメータに対応する形で、各パラメータの**勾配**をリストとして保持します（勾配については後述します）。\n",
    "\n",
    "これが本書の「実装ルール」です。\n",
    "\n",
    "ここでは順伝播のみの実装を考えているので、上記の「実装ルール」のうち、次の点だけに焦点を当てて実装を行います。  \n",
    "ひとつは「レイヤに <span style=\"font-family: Consolas\">forward()</span> メソッドを実装すること」そして「パラメータをインスタンス変数の <span style=\"font-family: Consolas\">params</span> にまとめること」です。  \n",
    "\n",
    "この実装ルールに従い、レイヤの実装を行います。まずは Sigmoid レイヤから実装します。  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "上のように、シグモイド関数をクラスとして実装し、メインの変換処理を <span style=\"font-family: Consolas\">forward(x)</span> メソッドとして実装します。  \n",
    "ここで、Sigmoid レイヤには学習するパラメータは存在しないので、インスタンス変数のparams は空のリストで初期化しています。\n",
    "\n",
    "続いて、全結合層である Affine レイヤの実装を示します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "Affine レイヤは、初期化時に重みとバイアスを受け取ります。  \n",
    "このとき、Affine レイヤのパラメータは、重みとバイアスになります（その２つのパラメータが、ニューラルネットワークの学習で随時更新されていきます）。  \n",
    "そこで、その２つをインスタンス変数の <span style=\"font-family: Consolas\">params</span> にリストとして保持します。後は、<span style=\"font-family: Consolas\">forward(x)</span> による順伝播の処理を実装します。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_2-5.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "それでは、上で実装したレイヤを使って、ニューラルネットワークの推論処理を実装します。  \n",
    "ここでは、図1-11 に示すレイヤ構成のニューラルネットワークを実装します。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-11_実装するニューラルネットワークのレイヤ構成.png)\n",
    "\n",
    "図1-11 に示すとおり、ここでは入力 $\\bold{x}$ が Affine レイヤ、Sigmoid レイヤ、Affine レイヤを経て、スコアである $\\bold{s}$ が出力されます。  \n",
    "私たちは、このニューラルネットワークを <span style=\"font-family: Consolas\">TwoLayerNet</span> という名前のクラスとして、メインの推論処理を <span style=\"font-family: Consolas\">predict(x)</span> というメソッドで実装することにします。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_2-6.png)\n",
    "\n",
    "それでは、<span style=\"font-family: Consolas\">TwoLayerNet</span> の実装を示します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "\n",
    "        # 重みとバイアスの初期化\n",
    "        W1 = np.random.randn(I, H)\n",
    "        b1 = np.random.randn(H)\n",
    "        W2 = np.random.randn(H, O)\n",
    "        b2 = np.random.randn(O)\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        \n",
    "        # すべての重みをリストにまとめる\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "このクラスのイニシャライザでは、  \n",
    "初めに 重みとバイアスを初期化 し、３つのレイヤを生成 します。  \n",
    "また、学習すべき重みパラメータを params リストにまとめて保持します。  \n",
    "ここでは、各レイヤのインスタンス変数の <span style=\"font-family: Consolas\">params</span> に学習パラメータが保持されているので、それらを連結するだけで完了です。  \n",
    "これで <span style=\"font-family: Consolas\">TwoLayerNet</span> の <span style=\"font-family: Consolas\">params</span> 変数には、すべての学習パラメータが存在することになりました。  \n",
    "このように、パラメータをひとつのリストにまとめることで、「パラメータの更新」や「パラメータの保存」が簡単に行えるようになります。\n",
    "\n",
    "なお、Python ではリストどうしの連結は＋演算子によって行えます。  \n",
    "簡単な例をひとつ示すと、次のようになります。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C', 'D']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['A', 'B']\n",
    "a += ['C', 'D']\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ここで示したように、リストどうしの足し算によって、リストが連結されます。  \n",
    "上の <span style=\"font-family: Consolas\">TwoLayerNet</span> の実装では、各レイヤの <span style=\"font-family: Consolas\">params</span> リストを加算することで、すべての学習パラメータをひとつのリストにまとめました。\n",
    "\n",
    "それでは、 <span style=\"font-family: Consolas\">TwoLayerNet</span> クラスを利用して、ニューラルネットワークの推論を行いましょう。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "これで入力データ <span style=\"font-family: Consolas\">**x**</span> に対して、スコア <span style=\"font-family: Consolas\">**s**</span> を求めることができました。  \n",
    "このように、レイヤとしてクラス化することで、ニューラルネットワークが容易に実装できます。  \n",
    "またこのとき、<span style=\"font-family: Consolas\">model.params</span> には学習すべきパラメータがひとつのリストにまとめられているので、続いて行うニューラルネットワークの学習が行いやすくなります。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "## **1.3 ニューラルネットワークの学習**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ニューラルネットワークは学習を行わなければ、\"良い推論\" はできません。  \n",
    "そのため、最初に学習を行い、その学習されたパラメータを利用して推論を行うという流れが一般的です。  \n",
    "推論とは、前節で見たような、多クラス分類などの問題に答えを出す作業です。  \n",
    "一方、ニューラルネットワークの学習は、最適なパラメータを見つける作業になります。  \n",
    "本節では、ニューラルネットワークの学習について見ていきます。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.3.1 損失関数**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ニューラルネットワークの学習には、学習がどれだけうまくいっているかを知るための「指標」が必要になります。\n",
    "\n",
    "一般的にそれは **損失**（loss）と呼ばれます。損失とは、学習段階のある時点におけるニューラルネットワークの性能を示す指標です。  \n",
    "損失は、教師データとニューラルネットワークの予測結果を元に、それが \"どれだけ悪いか\" をスカラとして算出したものになります。\n",
    "\n",
    "ニューラルネットワークの損失を求めるには **損失関数**（loss function）を使用します。  \n",
    "多クラス分類を行うニューラルネットワークの場合、損失関数として **交差エントロピー誤差**（Cross Entropy Error）を用いる場合が多くあります。  \n",
    "交差エントロピー誤差 は、ニューラルネットワークが出力する各クラスの「確率」と「教師ラベル」から求められます。\n",
    "\n",
    "それでは、私たちがこれまで扱ってきたニューラルネットワークに対して、損失を求めてみましょう。  \n",
    "ここでは、前節までのネットワークに対して、Softmax レイヤと Cross Entropy Error レイヤを新たに追加します。  \n",
    "このときのネットワーク構成を \"レイヤ視点\" で描くと、図1-12 のようになります。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-12_損失関数を適用したニューラルネットワークのレイヤ構成.png)\n",
    "\n",
    "図1-12 の $\\bold{x}$ は入力データ、$\\bold{t}$ は教師ラベル、$\\bold{L}$ は損失を表します。  \n",
    "このとき、 Softmax レイヤの出力は確率となり、Cross Entropy Error レイヤには確率と教師ラベルが入力されます。  \n",
    "それでは、Softmax 関数と交差エントロピー誤差について説明します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "まずは **Softmax 関数** についてですが、これは次の式で表されます。\n",
    "\n",
    "$ \\quad $  \n",
    "$ \\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\n",
    "    y_k = \\frac{\\exp{(s_k)}}{\\displaystyle\\sum_{i=1}^{n}{\\exp{(s_i)}}}\n",
    "\\qquad\\qquad (1.6) $\n",
    "\n",
    "$ \\quad $  \n",
    "出力が全部で $n$ 個あるときの、$k$ 番目の出力 $y_k$ を求める計算式を示しています。  \n",
    "この $y_k$ は、$k$ 番目のクラスに対応する Softmax 関数の出力です。\n",
    "\n",
    "式$(1.6)$ に示したように、Softmax 関数の分子はスコア $s_k$ の指数関数、分母はすべての入力信号の指数関数の和から構成されます。  \n",
    "Softmax 関数の出力の各要素は 0以上1以下の実数になり、その和は1になることから、Softmax 関数の出力は「確率」として解釈できます。  \n",
    "この「確率」が、続いて交差エントロピー誤差へ入力されます。\n",
    "\n",
    "**交差エントロピー誤差**は、次の式で表されます。\n",
    "\n",
    "$ \\quad $  \n",
    "$ \\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\n",
    "    L = -{\\sum_{k}{t_k\\log{y_k}}}\n",
    "\\qquad\\qquad (1.7) $\n",
    "\n",
    "$ \\quad $  \n",
    "ここで、$t_k$ は $k$ 番目のクラスに対応する教師ラベルです。$\\log$ は、ネイピア数 $e$ を底とする対数です。  \n",
    "教師ラベルについては、$\\bold{t} = (0, 0, 1)$ のように、one-hot ベクトルで表記します。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_3-1.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "また、ミニバッチ処理を考慮したとき、交差エントロピー誤差は次の式で表されます。  \n",
    "ここではデータが $N$ 個あるとして、$t_{nk}$ は $n$ 個目のデータの $k$ 次元目の値を意味します。  \n",
    "$y_{nk}$ はニューラルネットワークの出力、$t_{nk}$ は教師ラベルを表します。\n",
    "\n",
    "$ \\quad $  \n",
    "$ \\displaystyle\\qquad\\qquad\\qquad\\qquad\n",
    "    L = -\\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}\\log{y_{nk}}\n",
    "\\qquad\\qquad (1.8) $\n",
    "\n",
    "$ \\quad $  \n",
    "式$(1.8)$ は少し複雑に見えますが、これはひとつのデータに対する損失関数を表す式$(1.7)$ を、単に $N$ 個分のデータに拡張しただけです。  \n",
    "ただし、式$(1.8)$ では $N$ で割ることによって、１個あたりの「平均の損失関数」を求めます。  \n",
    "そのように平均化することで、ミニバッチのサイズに関係なく、いつでも統一した指標が得られます。  \n",
    "本書では、Softmax 関数と交差エントロピー誤差を計算するレイヤを Softmax with Loss レイヤとして実装することにします。  \n",
    "そのため、私たちの（学習時における）ニューラルネットワークは、図1-13 のようなレイヤ構成になります。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-13_SoftmaxwithLossレイヤを使用して損失を出力する.png)\n",
    "\n",
    "図1-13 のように、本書では Softmax with Loss レイヤを利用します。ここでは、その実装についての説明は省略します。 \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.3.2 微分と勾配**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ニューラルネットワークの学習の目標は、**損失をできるだけ小さくするパラメータを見つけること**です。  \n",
    "このとき重要になるのが「微分」であり「勾配」です。\n",
    "\n",
    "さて、ある関数 $y = f(x)$ があるとしましょう。このとき、$x$ に関する $y$ の微分は、$\\frac{dy}{dx}$ と書きます。  \n",
    "この $\\frac{dy}{dx}$ が意味することは、$x$ の値を \"少しだけ\" 変化させたとき ――より正確には、その「少しの変化」を極限までに小さくしたとき―― に、  \n",
    "$y$ の値がどれだけ変化するか、という「変化の度合い」です。\n",
    "\n",
    "たとえば、$y = x^2$ という関数があるとします。  \n",
    "このとき、この関数の微分は $\\frac{dy}{dx} = 2x$ となります。  \n",
    "そしてこの微分の結果は、各$x$ における変化の度合いを表します。  \n",
    "実際それは、図1-14 に示すように関数の「傾き」に相当します。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-14_微分は傾きを表す.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "図1-14 では $x$ というひとつの変数について微分を求めましたが、複数の変数（多変数）についても同様に微分を求めることができます。  \n",
    "\n",
    "たとえば、$L$ をスカラ、$\\bold{x}$ をベクトルとして、$L = f(\\bold{x})$ という関数があるとします。  \n",
    "このとき、$x_i$ に関する $L$ の微分は、$\\frac{∂L}{∂x_i}$ と書けます。  \n",
    "そして、ベクトルの他の要素の微分についても求めることができ、それは次のようにまとめることができます。\n",
    "\n",
    "$ \\quad $  \n",
    "$ \\displaystyle\\qquad\\qquad\\qquad\n",
    "    \\frac{∂L}{∂\\bold{x}} = \\left(\\frac{∂L}{∂x_1}, \\frac{∂L}{∂x_2}, \\dots, \\frac{∂L}{∂x_n}\\right)\n",
    "\\qquad\\qquad (1.9) $\n",
    "\n",
    "$ \\quad $  \n",
    "このように、ベクトルの各要素に関する微分をまとめたものを**勾配**（gradient）と呼びます。\n",
    "\n",
    "また行列についても、ベクトルの場合と同様に勾配を考えることができます。  \n",
    "たとえば、$\\bold{W}$ を $m × n$ の行列とした場合、$L = g(\\bold{W})$ という関数の勾配は次のようにまとめて書くことができます。\n",
    "\n",
    "$ \\quad $  \n",
    "$ \\displaystyle\\qquad\\qquad\\qquad\n",
    "    \\frac{∂L}{∂\\bold{W}} = \\begin{pmatrix}\n",
    "        \\frac{∂L}{∂W_{11}} & \\dots  & \\frac{∂L}{∂W_{1n}} \\\\\n",
    "        \\vdots             & \\ddots &                    \\\\\n",
    "        \\frac{∂L}{∂x_{m1}} &        & \\frac{∂L}{∂W_{mn}}\n",
    "    \\end{pmatrix}\n",
    "\\qquad\\qquad (1.10) $\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "式$(1.10)$ に示すように、$L$ の $\\bold{W}$ に関する勾配は行列としてまとめることができます。  \n",
    "ここで重要な点は、$\\bold{W}$ と $\\frac{∂L}{∂\\bold{W}}$ の形状が同じだということです。  \n",
    "この「行列とその勾配が同じ形状である」という性質を利用することで、パラメータの更新作業やチェインルールの実装が簡単に行えます。  \n",
    "（チェインルールについては、すぐ後に詳しく説明します）。\n",
    "\n",
    "![代替テキスト](../images/Section1/Caution_1_3-1_.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.3.3 チェインルール**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "学習時におけるニューラルネットワークは、学習データを与える と 損失を出力します。  \n",
    "ここで私たちが得たいものは、各パラメータに関する損失の勾配です。  \n",
    "その勾配が得られれば、それを使ってパラメータの更新を行うことができます。  \n",
    "では、**ニューラルネットワークの勾配**はどのように求めることができるのでしょうか。  \n",
    "\n",
    "ここで**誤差逆伝播法**（back-propagation）が登場します。\n",
    "\n",
    "誤差逆伝播法を理解する上で、キーとなるのが**チェインルール**（連鎖律）です。  \n",
    "チェインルールとは、合成関数に関する微分の法則です。  \n",
    "それでは、チェインルールについて学びましょう。\n",
    "\n",
    "ここでは例として、$y = f(x)$ と $z = g(y)$ という２つの関数について考えます。  \n",
    "そして $z = g(f(x))$ で表されるように、最終的な出力 $z$ は２つの関数によって計算されるとします。  \n",
    "このとき、この合成関数の微分　―― $x$ に関する $z$ の微分―― は次のように求めることができます。\n",
    "\n",
    "$ \\quad $  \n",
    "$ \\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\n",
    "    \\frac{∂z}{∂x} = \\frac{∂z}{∂y}\\frac{∂y}{∂x}\n",
    "\\qquad\\qquad (1.11)$\n",
    "\n",
    "$ \\quad $  \n",
    "式$(1.11)$ のように、$x$ に関する $z$ の微分は、$y = f(x)$ の微分と $z = g(y)$ の微分の積で求められます。  \n",
    "\n",
    "つまり、各関数の局所的な微分を計算できれば、その積によって最終的な全体の微分を求めることができるのです。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_3-2.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.3.4 計算グラフ**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "これから誤差逆伝播法について見ていきますが、ここではその前準備として「計算グラフ」について説明します。  \n",
    "計算グラフは、計算を視覚的に表すものです。早速ですが、計算グラフの例を図1-15 に示します。  \n",
    "\n",
    "![代替テキスト](../images/Section1/図1-15_計算グラフ.png)\n",
    "\n",
    "図1-15 で示すように、計算グラフはノードと矢印で図示します。  \n",
    "このとき、加算を「+」ノードで表し、変数 $x$ と $y$ を各矢印の上に書くことにします。  \n",
    "このように、計算グラフでは演算をノードで表し、その処理結果が順に流れます。これが計算グラフの「**順伝播**」です。  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "計算グラフを使えば、計算を視覚的に把握することができます。さらに、その勾配も直感的に求めることができます。  \n",
    "ここで重要なのが、勾配は順伝播とは逆方向に伝播させるということです。この逆方向の伝播が「**逆伝播**」です。  \n",
    "\n",
    "逆伝播の説明の前に、逆伝播が行われる全体像をより明確にしたいと思います。  \n",
    "ここで私たちは $z = x + y$ という計算を扱っていますが、その計算の前後には「何らかの計算」があることを想定します（図1-16）。  \n",
    "そして、最終的にスカラである $L$ が出力されると仮定します。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-16_加算ノードは複雑な計算の一部を構成する.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "私たちの目標とすることは、$L$ の微分（勾配）を各変数について求めることです。  \n",
    "そうすると、計算グラフの逆伝播は図1-17 のように書くことができます。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-17_計算グラフの逆伝播.png)\n",
    "\n",
    "図1-17 に示すとおり、逆伝播は赤い太線の矢印で描画し、矢印の下側に「伝播する値」を書くことにします。\n",
    "\n",
    "このとき「伝播する値」とは、最終的な出力 $L$ の各変数に関する微分になります。  \n",
    "この例では、$z$ に関する微分は $\\frac{∂L}{∂z}$ であり、$x$ と $y$ に関する微分は、それぞれ $\\frac{∂L}{∂x}$ と $\\frac{∂L}{∂y}$ になります。  \n",
    "\n",
    "ここでチェインルールが登場します。  \n",
    "逆伝播で流れる微分の値は、上流（出力側）からの微分と各演算ノードの局所的な微分との積で計算されます。  \n",
    "そのため、上の例では、$\\frac{∂L}{∂x} = \\frac{∂L}{∂z}\\frac{∂z}{∂x}$ であり、$\\frac{∂L}{∂y} = \\frac{∂L}{∂z}\\frac{∂z}{∂y}$ となります。\n",
    "\n",
    "そして、私たちは $z = x + y$ という加算ノードによる演算を扱っており、$\\frac{∂z}{∂x} = 1$、$\\frac{∂z}{∂y} = 1$ が求められます。  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "そのため加算ノードでは、図1-18 のように、上流から伝わる伝播に1を乗算して下流へと勾配を伝播します。  \n",
    "つまり、上流からの勾配をそのまま流すだけになります。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-18_加算ノードの順伝播と逆伝播.png)\n",
    "\n",
    "このように計算グラフは計算を視覚的に表します。  \n",
    "そして、逆伝播による勾配の流れを見ることで、その導出の過程を理解するのにも役立てることができます。\n",
    "\n",
    "計算グラフを構築する演算ノードには、ここで見てきた「加算ノード」の他にもさまざまな演算が考えられます。  \n",
    "代表的な演算ノードをいくつか紹介します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "#### **1.3.4.1 乗算ノード**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "乗算ノードは、$z = x × y$ という計算です。このとき微分は、$\\frac{∂z}{∂x} = y$、$\\frac{∂z}{∂y} = x$ とそれぞれ求めることができます。  \n",
    "そのため、乗算ノードの逆伝播は図1-19 のように、「上流から伝わる勾配」に「順伝播時の入力を入れ替えた値」を乗算します。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-19_乗算ノードの順伝播と逆伝播.png)\n",
    "\n",
    "なお、これまで見てきた加算ノードと乗算ノードの説明では、ノードを流れるデータは「１変数」でした。  \n",
    "しかし、流れるデータは１変数ではなく、多変数 ――ベクトルや行列、テンソル―― であっても問題ありません。  \n",
    "加算ノード（もしくは乗算ノード）を流れるデータがテンソルの場合は、テンソルの各要素を独立に計算するだけです。  \n",
    "つまりその場合は、テンソルの他の要素とは独立に「要素ごとの（element-wise）演算」を行います。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "#### **1.3.4.2 分岐ノード**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "分岐ノードは、図1-20 に示すように、分岐するノードです。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-20_分岐ノードの順伝播と逆伝播.png)\n",
    "\n",
    "分岐ノードは、正確にはノードとしては描画せずに、単に線が２つに分かれるように描画します。  \n",
    "このとき、同じ値がコピーされて分岐します。  \n",
    "そのため、分岐ノードは「コピーノード」と呼ぶこともできます。  \n",
    "そしてその逆伝播は、図1-20 に示すとおり、上流からの勾配の「和」になります。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "#### **1.3.4.3 Repeatノード**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "分岐ノードは２つの分岐でしたが、これを一般化させると $N$ 個の分岐（コピー）が考えられます。  \n",
    "ここではそれを Repeat ノードと呼ぶことにします。それでは、Repeat ノードの例を計算グラフで書いてみましょう。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-21_Repeatノードの順伝播と逆伝播.png)\n",
    "\n",
    "図1-21 に示すとおり、ここでは長さが $D$ の配列を $N$ 個だけ複製する例を示しています。  \n",
    "この Repeat ノードは、$N$ 個の分岐ノードとみなすことができるため、その逆伝播は $N$ 個の勾配の総和として求めることができます。  \n",
    "実際に実装例を挙げると、次のように書くことができます。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, N = 4, 3\n",
    "np.random.seed(100)\n",
    "x = np.random.randn(1, D)   # 入力(1*D)\n",
    "y = np.repeat(x, N, axis=0) # xを入力したRepeatノードの出力(N*D) 順伝播\n",
    "dy = np.random.randn(N, D)  # 仮の勾配\n",
    "dx = np.sum(dy, axis=0, keepdims=True) # 逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "[[-1.74976547  0.3426804   1.1530358  -0.25243604]]\n",
      "\n",
      "y: xの複製\n",
      "[[-1.74976547  0.3426804   1.1530358  -0.25243604]\n",
      " [-1.74976547  0.3426804   1.1530358  -0.25243604]\n",
      " [-1.74976547  0.3426804   1.1530358  -0.25243604]]\n"
     ]
    }
   ],
   "source": [
    "print(\"x\")\n",
    "print(x)\n",
    "print(\"\\ny: xの複製\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy: 上流から伝播してきた勾配\n",
      "[[ 0.98132079  0.51421884  0.22117967 -1.07004333]\n",
      " [-0.18949583  0.25500144 -0.45802699  0.43516349]\n",
      " [-0.58359505  0.81684707  0.67272081 -0.10441114]]\n",
      "\n",
      "dx: repeatノードの逆伝播\n",
      "[[ 0.20822991  1.58606736  0.43587349 -0.73929099]]\n",
      "\n",
      "0列目の合計\n",
      "0.9813207869512316 + -0.18949583082317534 + -0.5835950503226648 = 0.20822990580539147\n"
     ]
    }
   ],
   "source": [
    "print(\"dy: 上流から伝播してきた勾配\")\n",
    "print(dy)\n",
    "print(\"\\ndx: repeatノードの逆伝播\")\n",
    "print(dx)\n",
    "print(\"\\n0列目の合計\")\n",
    "print(f\"{dy[0][0]} + {dy[1][0]} + {dy[2][0]} = {dy[0][0] + dy[1][0] + dy[2][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ここでは、<span style=\"font-family: Consolas\">np.repeat()</span> メソッドによって、要素の複製を行います。  \n",
    "上の例では配列 <span style=\"font-family: Consolas\">x</span> を <span style=\"font-family: Consolas\">N</span> 回複製しますが、このとき <span style=\"font-family: Consolas\">axis</span> を指定することで、どの軸方向に沿って複製するかを指定できます。  \n",
    "また、逆伝播では総和を求めることになるので、NumPy の <span style=\"font-family: Consolas\">sum()</span> メソッドを利用します。  \n",
    "このときも、引数の <span style=\"font-family: Consolas\">axis</span> を指定することで、どの軸方向に沿って和を求めるかを指定します。  \n",
    "さらに引数で <span style=\"font-family: Consolas\">keepdims=True</span> を指定することで、２次元配列の次元数を維持します。  \n",
    "上の例では、<span style=\"font-family: Consolas\">keepdims=True</span> の場合、<span style=\"font-family: Consolas\">np.sum()</span> の結果の形状は <span style=\"font-family: Consolas\">(1, D) になり</span>、<span style=\"font-family: Consolas\">keepdims=False</span> の場合は <span style=\"font-family: Consolas\">(D,)</span> になります。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_3-3.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "#### **1.3.4.4 Sumノード**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "Sum ノード（総和ノード）は汎用的な加算ノードです。  \n",
    "ここでは $N × D$ の配列に対して、その総和を第０軸に対して求める計算を考えます。  \n",
    "このとき、Sum ノードの順伝播と逆伝播は図1-22 のようになります。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-22_Sumノードの順伝播と逆伝播.png)\n",
    "\n",
    "図1-22 に示すとおり、Sum ノードの逆伝播は、上流からの勾配をすべての矢印に分配します。  \n",
    "これは加算ノードの逆伝播の自然な拡張です。  \n",
    "それでは、Repeat ノードと同様に、Sum ノードの実装例も示しましょう。これは次のようになります。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, N = 4, 3\n",
    "np.random.seed(100)\n",
    "x = np.random.randn(N, D) # 入力\n",
    "y = np.sum(x, axis=0, keepdims=True) # forward\n",
    "dy = np.random.randn(1, D) # 仮の勾配\n",
    "dx = np.repeat(dy, N, axis=0) # backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "[[-1.74976547  0.3426804   1.1530358  -0.25243604]\n",
      " [ 0.98132079  0.51421884  0.22117967 -1.07004333]\n",
      " [-0.18949583  0.25500144 -0.45802699  0.43516349]]\n",
      "y: xの総和\n",
      "[[-0.95794052  1.11190069  0.91618849 -0.88731588]]\n"
     ]
    }
   ],
   "source": [
    "print(\"x\")\n",
    "print(x)\n",
    "print(\"y: xの総和\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy: 上流から流れてきた勾配\n",
      "[[-0.58359505  0.81684707  0.67272081 -0.10441114]]\n",
      "dx: sumノードの逆伝播\n",
      "[[-0.58359505  0.81684707  0.67272081 -0.10441114]\n",
      " [-0.58359505  0.81684707  0.67272081 -0.10441114]\n",
      " [-0.58359505  0.81684707  0.67272081 -0.10441114]]\n"
     ]
    }
   ],
   "source": [
    "print(\"dy: 上流から流れてきた勾配\")\n",
    "print(dy)\n",
    "print(\"dx: sumノードの逆伝播\")\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "見てのとおり、Sum ノードの順伝播は <span style=\"font-family: Consolas\">np.sum()</span> メソッド、逆伝播は <span style=\"font-family: Consolas\">np.repeat()</span> メソッドによって実装できます。  \n",
    "ここでの興味深い点は、Sum ノードと Repeat ノードは、それぞれ「逆の関係」にあるということです。  \n",
    "逆の関係とは、Sum ノードの順伝播が Repeat ノードの逆伝播に相当し、Sum ノードの逆伝播が Repeat ノードの順伝播に相当するということです。\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "#### **1.3.4.5 MatMulノード**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "本書では、行列の積を計算するノードを **MatMul ノード**（「Matrix Multiply」の略）とします。  \n",
    "MatMul ノードの逆伝播はやや複雑になるため、ここでは一般的な説明を行った後で、直感的な理解が得られる説明を行います。\n",
    "  \n",
    "MatMul ノードを説明するにあたり、$\\bold{y} = \\bold{xW}$ という計算を考えます。  \n",
    "ここで、$\\bold{x}$、$\\bold{W}$、$\\bold{y}$ の形状は、それぞれ $1×D$、$D×H$、$1×H$ とします（図1-23）。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-23_MatMulノードの順伝播.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "このとき、$\\bold{x}$ の $i$ 番目の要素に関する微分 $\\frac{∂L}{∂x_i}$ は、次のように求められます。\n",
    "\n",
    "$ \\quad $  \n",
    "$ \\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\n",
    "    \\frac{∂L}{∂x_i} = \\sum_j\\frac{∂L}{∂y_j}\\frac{∂y_j}{∂x_i}\n",
    "\\qquad\\qquad (1.12)$\n",
    "\n",
    "$ \\quad $  \n",
    "式$(1.12)$ の $\\frac{∂L}{∂x_i}$ は、$x_i$ を少しだけ変化させたときに $L$ がどれだけ変化するか、という変化の度合いです。\n",
    "\n",
    "$x_i$ を変化させると、ベクトル $\\bold{y}$ の \"すべての要素\" が変化します。さらに、$\\bold{y}$ の各要素の変化を通じて、最終的に $L$ が変化します。  \n",
    "そのため、$x_i$ から $L$ にいたるチェインルールの経路は複数あり、その総和が $\\frac{∂L}{∂x_i}$ となります。\n",
    "\n",
    "さて式$(1.12)$ ですが、これはまだ簡単にすることができます。  \n",
    "それには、$\\frac{∂y_j}{∂x_i} = W_{ij}$ が成り立つことを利用して、それを式$(1.12)$ に代入します。\n",
    "\n",
    "$ \\quad $  \n",
    "$ \\displaystyle\\qquad\\qquad\\qquad\\qquad\n",
    "    \\frac{∂L}{∂x_i} = \\sum_j\\frac{∂L}{∂y_j}\\frac{∂y_j}{∂x_i} = \\sum_j\\frac{∂L}{∂y_j}W_{ij}\n",
    "\\qquad (1.13)$\n",
    "\n",
    "$ \\quad $  \n",
    "式$(1.13)$ から、$\\frac{∂L}{∂x_i}$ は、「ベクトル $\\frac{∂L}{∂\\bold{y}}$」と「$\\bold{W}$ の $i$ 行目のベクトル」の内積によって求められることが分かります。  \n",
    "この関係から、次の式が導けます。\n",
    "\n",
    "$ \\quad $  \n",
    "$ \\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\n",
    "    \\frac{∂L}{∂\\bold{x}} = \\frac{∂L}{∂\\bold{y}}\\bold{W}^T\n",
    "\\qquad\\qquad (1.14)$\n",
    "\n",
    "$ \\quad $  \n",
    "式$(1.14)$ が示すように、$\\frac{∂L}{∂\\bold{x}}$ は行列の積によって一度に求められます。ここで、$\\bold{W}^T$ の $T$ は転置行列であることを表します。  \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "それでは式$(1.14)$ に対して「形状チェック」を行ってみましょう。  \n",
    "結果は図1-24 のようになります。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-24_行列の積の形状チェック.png)\n",
    "\n",
    "図1-24 より、行列の形状の推移は正しいことが分かります。  \n",
    "これによって、式$(1.14)$ が正しい計算であることが確認できるのです。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "また、それを逆手に取って  ――つまり、その整合性が成り立つようにすることで――、逆伝播の数式を導くこともできます。  \n",
    "その方法を説明するにあたって、再度 $\\bold{y} = \\bold{xW}$ という行列の積の計算を考えます。  \n",
    "ただし今回はミニバッチ処理を考慮して、$\\bold{x}$ には $N$ 個のデータが格納されているものとします。  \n",
    "このとき、$\\bold{x}$、$\\bold{W}$、$\\bold{y}$ の形状はそれぞれ $N×D$、$D×H$、$N×H$ となり、逆伝播の計算グラフは図1-25 のようになります。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-25_MatMulノードの逆伝播.png)\n",
    "\n",
    "それでは $\\frac{∂L}{∂\\bold{x}}$ は、どのように計算できるか考えましょう。  \n",
    "このとき、$\\frac{∂L}{∂\\bold{x}}$ に関係する変数（行列）は、上流からの勾配 $\\frac{∂L}{∂\\bold{y}}$ と $\\bold{W}$ です。  \n",
    "$\\bold{W}$ が関係するのは、行列の積の逆伝播も、乗算の逆伝播のように「順伝播時の入力を入れ替えた値」を使用するためです。  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "後は、それぞれの行列の形状に注目し、その整合性が保たれるように行列の積を組み立てることにします。  \n",
    "そうすると、図1-26 のように行列の積の逆伝播が導けます。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-26_行列の形状を確認することで逆伝播の式を導く.png)\n",
    "\n",
    "図1-26 に示すとおり、行列の形状を確認することで、行列の積の逆伝播の式を組み立てることができました。  \n",
    "これで、MatMul ノードの逆伝播が導けました！ それでは、MatMul ノードをレイヤとして実装しましょう。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]  # Wと同じ形状のゼロ行列\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "MatMul レイヤは <span style=\"font-family: Consolas\">params</span> に学習するパラメータを保持します。  \n",
    "そしてそれに対応させる形で、勾配を <span style=\"font-family: Consolas\">grads</span> に保持させることにします。  \n",
    "逆伝播では <span style=\"font-family: Consolas\">dx</span> と <span style=\"font-family: Consolas\">dW</span> を求めて、重みの勾配をインスタンス変数の <span style=\"font-family: Consolas\">grads</span> に設定します。  \n",
    "なお、勾配の値を設定する際に、<span style=\"font-family: Consolas\">grads[0][...] = dW</span> というように「３点リーダー」を使っています。  \n",
    "この「３点リーダー」を用いることで、NumPy 配列のメモリ位置を固定した上で、NumPy 配列の要素を上書きします。\n",
    "\n",
    "「３点リーダー」について、具体例を出して説明します。ここに２つの NumPy 配列 <span style=\"font-family: Consolas\">a</span> と <span style=\"font-family: Consolas\">b</span> があります。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n",
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "a = b\n",
    "print(a)\n",
    "\n",
    "a[...] = b\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "<span style=\"font-family: Consolas\">a = b</span> と <span style=\"font-family: Consolas\">a[...] = b</span> の場合では、<span style=\"font-family: Consolas\">a</span> にはともに <span style=\"font-family: Consolas\">[4, 5, 6]</span> が代入されますが、<span style=\"font-family: Consolas\">a</span> が指すメモリの位置が異なります。  \n",
    "実際に（単純化した）メモリを可視化すると、図1-27 のようになります。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-27_aイコールbとa３点リーダーイコールbの違い.png)\n",
    "\n",
    "図1-27 に示すように、<span style=\"font-family: Consolas\">a = b</span> では、<span style=\"font-family: Consolas\">a</span> が指すメモリ位置が <span style=\"font-family: Consolas\">b</span> と同じになります。  \n",
    "実際のデータ（<span style=\"font-family: Consolas\">4,5,6</span>）は複製されないため、これは「浅いコピー」と言えます。\n",
    "\n",
    "一方、<span style=\"font-family: Consolas\">a[...] = b</span> のときは、<span style=\"font-family: Consolas\">a</span> のメモリ位置は固定のまま、<span style=\"font-family: Consolas\">a</span> の指すメモリ上に <span style=\"font-family: Consolas\">b</span> の要素がコピーされます。  \n",
    "これは実際のデータが複製されるため、「深いコピー」と言えます。\n",
    "\n",
    "以上より「３点リーダー」を用いることで、変数のメモリ位置を固定できることが分かりました。  \n",
    "このメモリ位置を固定することによって、インスタンス変数の <span style=\"font-family: Consolas\">grads</span> の扱いがよりシンプルになります。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_3-4.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.3.5 勾配の導出と逆伝播の実装**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "計算グラフの説明が終わったので、続いて実用的なレイヤを実装しましょう。  \n",
    "ここでは、Sigmoid レイヤ、全結合層のAffine レイヤ、Softmax with Loss レイヤを実装します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "#### **1.3.5.1 Sigmoid レイヤ**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "シグモイド関数は、$y = \\frac{1}{1+\\exp(−x)}$ という式で表されます。  \n",
    "そして、シグモイド関数の微分は、次の式で表されます。\n",
    "\n",
    "$ \\qquad $  \n",
    "$ \\displaystyle\\qquad\\qquad\\qquad\\qquad\n",
    "    \\frac{∂y}{∂x} = y(1 − y)\n",
    "\\qquad\\qquad(1.15)$\n",
    "\n",
    "$ \\quad $  \n",
    "式$(1.15)$ より、Sigmoid レイヤの計算グラフは図1-28 のように書くことができます。  \n",
    "ここでは、出力側のレイヤから伝わってきた勾配 $(\\frac{∂L}{∂y})$ にSigmoid 関数の微分 $(\\frac{∂y}{∂x})$ をかけて、それを入力側のレイヤに伝播します。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-28_Sigmoidレイヤの計算グラフ.png)\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_3-5.png)\n",
    "\n",
    "それでは、Sigmoid レイヤをPython で実装します。図1-28 を参考にすれば、次のように実装することができます。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out  # ∂L/∂x = ∂L/∂y * ∂y/∂x\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ここでは、順伝播の出力をインスタンス変数の <span style=\"font-family: Consolas\">out</span> に保持しておきます。  \n",
    "そして、逆伝播において、その <span style=\"font-family: Consolas\">out</span> 変数を使って計算を行います。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "#### **1.3.5.2 Affine レイヤ**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "前に示したとおり、Affine レイヤの順伝播は、<span style=\"font-family: Consolas\">y = np.dot(x, W) + b</span> で実装できました。  \n",
    "このときバイアスの加算では、NumPy のブロードキャストが使われています。  \n",
    "その点を明示的に表すと、Affine レイヤの計算グラフは図1-29 のように書けます。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-29_Affinレイヤの計算グラフ.png)\n",
    "\n",
    "図1-29 に示すとおり、MatMul ノードで行列の積の計算を行います。  \n",
    "そして、バイアスが Repeat ノードで複製され、その後で加算が行われます。\n",
    "  \n",
    "それでは、Affine レイヤの実装を示します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "本書の実装ルールに従い、インスタンス変数の <span style=\"font-family: Consolas\">params</span> にはパラメータを、<span style=\"font-family: Consolas\">grads</span> には勾配を保持します。\n",
    "\n",
    "逆伝播の実装は、MatMul ノードと Repeat ノードの逆伝播を行えば求められます。  \n",
    "Repeat ノードの逆伝播は <span style=\"font-family: Consolas\">np.sum()</span> によって計算できますが、このとき行列の形状に注目することで、どの軸（<span style=\"font-family: Consolas\">axis</span>）で和を求めるべきかがはっきりします。  \n",
    "最後に、重みパラメータの勾配をインスタンス変数の <span style=\"font-family: Consolas\">grads</span> に設定します。  \n",
    "以上が、Affine レイヤの実装です。\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "#### **1.3.5.3 Softmax with Loss レイヤ**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "私たちは、Softmax 関数と交差エントロピー誤差を合わせて Softmax with Loss レイヤとして実装することにします。  \n",
    "このとき、計算グラフは図1-30 のようになります。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-30_SoftmaxwithLossレイヤの計算グラフ.png)\n",
    "\n",
    "図1-30 の計算グラフでは、Softmax 関数は Softmax レイヤとして、交差エントロピー誤差は Cross Entropy Error レイヤとして表記します。  \n",
    "ここでは、３クラス分類を行う場合を想定し、前レイヤ（=入力層に近い側のレイヤ）から３つの入力を受け取ることを仮定します。\n",
    "\n",
    "図1-30 に示すように、Softmax レイヤは、入力である $(a_1, a_2, a_3)$ を正規化して、$(y_1, y_2, y_3)$ を出力します。  \n",
    "Cross Entropy Error レイヤは、Softmax の出力 $(y_1, y_2, y_3)$ と教師ラベルの $(t_1, t_2, t_3)$ を受け取り、それらのデータから損失 $L$ を出力します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ここでは、Softmax with Loss レイヤの実装については説明を省略します。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_3-6.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.3.6 重みの更新**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "誤差逆伝播法によって勾配を求めることができたら、その勾配を使ってニューラルネットワークのパラメータを更新します。  \n",
    "このとき、ニューラルネットワークの学習は次の手順で行います。\n",
    "\n",
    "- Step-1（ミニバッチ）  \n",
    "    訓練データの中からランダムに複数のデータを選び出す\n",
    "- Step-2（勾配の算出）  \n",
    "    誤差逆伝播法により、各重みパラメータに関する損失関数の勾配を求める\n",
    "- Step-3（パラメータの更新）  \n",
    "    勾配を使って重みパラメータを更新する\n",
    "- Step-4（繰り返す）  \n",
    "    Step-1、Step-2、Step-3 を必要な回数だけ繰り返す\n",
    "\n",
    "まずはミニバッチでデータを選び、続いて誤差逆伝播法によって重みの勾配を得ます。  \n",
    "この勾配は、現時点での重みパラメータにおいて、損失を最も増やす方向を指します。  \n",
    "そのため、パラメータをその勾配の逆方向に更新することで、損失を下げることができるのです。これが**勾配降下法**（Gradient Descent）です。  \n",
    "後は、その作業を必要な回数だけ繰り返し行います。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "Step-3 では、重みを更新しますが、この重みの更新方法としてはさまざまな手法が提案されています。  \n",
    "ここでは、その中でも最も単純な SGD（Stochastic Gradient Descent：確率的勾配降下法）という手法を実装します。  \n",
    "なお、「Stochastic（確率的）」は、ランダムに選ばれたデータ（ミニバッチ）に対する勾配を用いることを意味します。  \n",
    "SGD は単純な方法で、（現状の）重みを勾配方向へある一定の距離だけ更新します。  \n",
    "数式で表すと、式$(1.16)$ のようになります。\n",
    "\n",
    "$ \\quad $  \n",
    "$ \\displaystyle\\qquad\\qquad\\qquad\\qquad\n",
    "    \\bold{W} \\leftarrow \\bold{W} - \\eta\\frac{∂L}{∂\\bold{W}}\n",
    "\\qquad\\qquad (1.16)$\n",
    "\n",
    "$ \\quad $  \n",
    "更新する重みパラメータを $\\bold{W}$ とし、$\\bold{W}$ に関する損失関数の勾配を $\\frac{∂L}{∂\\bold{W}}$ とします。  \n",
    "$\\eta$ は学習係数を表し、実際には 0.01 や 0.001 といった値をあらかじめ決めて使います。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "それでは、SGD の実装に移りましょう。  \n",
    "ここではモジュール化を考慮して、パラメータの更新を行うクラスを <span style=\"font-family: Consolas\">common/optimizer.py</span> に実装することにします。  \n",
    "（このファイルでは、SGD の他にもAdaGrad やAdam などの実装も行います）。  \n",
    "\n",
    "そして、パラメータの更新を行うクラスは <span style=\"font-family: Consolas\">update(params, grads)</span> という共通のメソッドを持つように実装します。  \n",
    "引数の <span style=\"font-family: Consolas\">params</span> にはニューラルネットワークの重みが、<span style=\"font-family: Consolas\">grads</span> には勾配がそれぞれリストとして格納されているものとします。  \n",
    "そして、<span style=\"font-family: Consolas\">params</span> と <span style=\"font-family: Consolas\">grads</span> の同じインデックスには、対応するパラメータと勾配がそれぞれ格納されていることを想定します。  \n",
    "そうすると、SGD は次のように実装できます。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "初期化の引数である <span style=\"font-family: Consolas\">lr</span> は learning rate（学習係数）を表します。  \n",
    "ここでは、この学習係数をインスタンス変数として保持します。  \n",
    "そして、<span style=\"font-family: Consolas\">update(params, grads)</span> というメソッドの中で、パラメータの更新処理を実装します。  \n",
    "この <span style=\"font-family: Consolas\">SGD</span> というクラスを使えば、ニューラルネットワークのパラメータの更新は、次のように行うことができます（次に示すコードは、実際には動作しない擬似コードです）。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(...)\n",
    "optimizer = SGD()\n",
    "for i in range(10000):\n",
    "    ...\n",
    "    x_batch, t_batch = get_mini_batch(...) # ミニバッチの取得\n",
    "    loss = model.forward(x_batch, t_batch)\n",
    "    model.backward()\n",
    "    optimizer.update(model.params, model.grads)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "このように、最適化を行うクラスを分離して実装することで、機能のモジュール化が容易になります。\n",
    "\n",
    "本書では、SGD の他にも、Momentum や AdaGrad、Adam などの手法を実装していますが、ここではそれらの最適化手法についての説明は省略します。  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 次ここから"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "## **1.4 ニューラルネットワークで問題を解く**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "以上で準備が整いました。これから簡単なデータセットに対して、ニューラルネットワークの学習を行います。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.4.1 スパイラル・データセット**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "本書ではデータセットのための便利なクラスを <span style=\"font-family: Consolas\">dataset</span> ディレクトリにいくつか提供しています。  \n",
    "本節ではその中から <span style=\"font-family: Consolas\">dataset/spiral.py</span> というファイルを利用します。  \n",
    "このファイルは「スパイラル（渦巻き）データ」を読み込むクラスが実装されており、次のように利用します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (300, 2)\n",
      "t (300, 3)\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append('..')\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset_from_git import spiral\n",
    "\n",
    "x, t = spiral.load_data()\n",
    "print('x', x.shape)\n",
    "print('t', t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "spiral.load_data() によってデータの読み込みを行っています。  \n",
    "このとき、<span style=\"font-family: Consolas\">x</span> が入力データで <span style=\"font-family: Consolas\">t</span> が教師ラベルです。  \n",
    "<span style=\"font-family: Consolas\">x</span> と <span style=\"font-family: Consolas\">t</span> の形状を見ると、それぞれ300個のサンプルデータがあり、<span style=\"font-family: Consolas\">x</span> は２次元データ、<span style=\"font-family: Consolas\">t</span> は３次元データであることが分かります。  \n",
    "なお、<span style=\"font-family: Consolas\">t</span> は <span style=\"font-family: Consolas\">one-hot</span> ベクトルであり、対応するクラスが1に、それ以外は0にラベル付けされています。  \n",
    "それでは、このデータをグラフ上にプロットしてみましょう。結果は図1-31 のようになります。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 図1-31\n",
    "N = 100\n",
    "CLS_NUM = 3\n",
    "markers = ['o', 'x', '^']\n",
    "for i in range(CLS_NUM):\n",
    "    plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-31_学習に使用するスパイラルデータセット.png)\n",
    "\n",
    "図1-31 に示すように、入力は２次元のデータで、分類するクラス数は３つあります。  \n",
    "これは、直線では分離できないため、非線形な分離線を学習する必要があります。\n",
    "\n",
    "私たちのニューラルネットワーク\n",
    "\n",
    "――活性化関数に非線形な Sigmoid 関数を使用した隠れ層のあるニューラルネットワーク――\n",
    "\n",
    "は、その非線形なパターンを正しく学習できるのでしょうか？  \n",
    "早速、実験してみましょう。\n",
    "\n",
    "![代替テキスト](../images/Section1/Caution_1_4-1.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.4.2 ニューラルネットワークの実装**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "それでは、ニューラルネットワークの実装を行います。ここでは隠れ層がひとつ\n",
    "のニューラルネットワークを実装します。  \n",
    "まずは、インポート文とイニシャライザの__init__() を示します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.layers import Affine, Sigmoid, SoftmaxWithLoss\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        # 重みとバイアスの初期化\n",
    "        W1 = 0.01 * np.random.randn(I, H)\n",
    "        b1 = np.zeros(H)\n",
    "        W2 = 0.01 * np.random.randn(H, O)\n",
    "        b2 = np.zeros(O)\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "イニシャライザは３つの引数を受け取ります。  \n",
    "<span style=\"font-family: Consolas\">input_size</span> は入力層のニューロンの数、<span style=\"font-family: Consolas\">hidden_size</span> は中間層のニューロンの数、そして <span style=\"font-family: Consolas\">output_size</span> は出力層のニューロンの数を表します。\n",
    "\n",
    "中身の実装では、まずはバイアスを0（ゼロ）ベクトル（<span style=\"font-family: Consolas\">np.zeros()</span>）で初期化します。  \n",
    "そして、重みを小さなランダム値（<span style=\"font-family: Consolas\">0.01 * np.random.randn()</span>）で初期化します。  \n",
    "なお、重みを小さなランダム値にすることで、学習がうまく進みやすくなります。  \n",
    "続いて、必要なレイヤを生成し、それをインスタンス変数の <span style=\"font-family: Consolas\">layers</span> リストにまとめます。  \n",
    "最後に、このモデルで使用するパラメータと勾配をひとつにまとめます。\n",
    "\n",
    "![代替テキスト](../images/Section1/Caution_1_4-2.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "続いて、<span style=\"font-family: Consolas\">TwoLayerNet</span> に３つのメソッドを実装します。  \n",
    "ここでは、推論を行う <span style=\"font-family: Consolas\">predict()</span> メソッド、順伝播の <span style=\"font-family: Consolas\">forward()</span> メソッド、逆伝播の <span style=\"font-family: Consolas\">backward()</span> メソッドを実装します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    for layer in self.layers:\n",
    "        x = layer.forward(x)\n",
    "    return x\n",
    "\n",
    "def forward(self, x, t):\n",
    "    score = self.predict(x)\n",
    "    loss = self.loss_layer.forward(score, t)\n",
    "    return loss\n",
    "\n",
    "def backward(self, dout=1):\n",
    "    dout = self.loss_layer.backward(dout)\n",
    "    for layer in reversed(self.layers):\n",
    "        dout = layer.backward(dout)\n",
    "    return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "見てのとおり、ここでの実装はスッキリとしたものになりました！  \n",
    "私たちはすでにニューラルネットワークで使用する処理ブロックを「レイヤ」として実装してきたので、  \n",
    "ここでは、それらのレイヤの <span style=\"font-family: Consolas\">forward()</span> と <span style=\"font-family: Consolas\">backward()</span> を適切な順番で呼ぶだけなのです。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.4.3 学習用のソースコード**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "続いて、学習を行うコードを示します。  \n",
    "ここでは、学習データを読み込み、ニューラルネットワーク（モデル）とオプティマイザを生成します。  \n",
    "そして、先ほど示した学習の４ステップの手順に従って学習を行います。  \n",
    "なお、機械学習の分野では、問題のために設計した手法（ニューラルネットワークや SVM など）を指して、「モデル」と呼ぶことが一般的です。  \n",
    "それでは、学習用のコードを次に示します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.optimizer import SGD\n",
    "from dataset import spiral\n",
    "import matplotlib.pyplot as plt\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 1. ハイパーパラメータの設定\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "\n",
    "# 2. データの読み込み、モデルとオプティマイザの生成\n",
    "x, t = spiral.load_data()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "# 学習で使用する変数\n",
    "data_size = len(x)\n",
    "max_iters = data_size // batch_size\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "loss_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    # 3. データのシャッフル\n",
    "    idx = np.random.permutation(data_size)\n",
    "    x = x[idx]\n",
    "    t = t[idx]\n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "        batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "        # 4. 勾配を求め、パラメータを更新\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        \n",
    "        # 5. 定期的に学習経過を出力\n",
    "        if (iters+1) % 10 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            print('| epoch %d | iter %d / %d | loss %.2f'\n",
    "            % (epoch + 1, iters + 1, max_iters, avg_loss))\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "まずはコードの１.の場所で、ハイパーパラメータを設定します。  \n",
    "具体的には、学習するエポック数、バッチサイズや隠れ層のニューロン数、学習係数を設定します。  \n",
    "続いて２.の場所で、データの読み込みを行い、ニューラルネットワーク（モデル）とオプティマイザを生成します。  \n",
    "私たちはすでに２層のニューラルネットワークを <span style=\"font-family: Consolas\">TwoLayerNet</span> クラスとして、またオプティマイザを <span style=\"font-family: Consolas\">SGD</span> クラスとして実装しました。ここではそれらを利用します。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_4-1.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "学習を行う際には、ミニバッチとしてランダムにデータを選びます。  \n",
    "ここでは、エポック単位でデータのシャッフルを行い、シャッフルを行ったデータに対して、頭から順にデータを抜き出します。  \n",
    "データのシャッフルには――正確には、データの「インデックス」のシャッフルには――、np.random.permutation() メソッドを使います。  \n",
    "これは引数に N を与えると、0 から N − 1 までのランダムな並びを作成して返します。  \n",
    "実際の使用例は次のようになります。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 1, 6, 4, 7, 0, 5, 2, 3, 9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.permutation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 9, 8, 5, 7, 2, 1, 3, 0, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.permutation(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "このように、<span style=\"font-family: Consolas\">np.random.permutation()</span> を呼ぶと、データのインデックスを無作為にシャッフルすることができます。  \n",
    "続いてコードの ４.で勾配を求め、パラメータを更新します。  \n",
    "最後に ５.で、定期的に学習の結果を出力します。  \n",
    "ここでは、10 イテレーションごとに損失の平均を求め、それを変数の loss_list に追加します。  \n",
    "以上が学習を行うためのソースコードです。\n",
    "\n",
    "![代替テキスト](../images/Section1/Caution_1_4-3.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "それでは、上のコードを実行してみましょう。  \n",
    "そうすると、ターミナルに出力される損失の値が順調に下がっていくことが分かります。  \n",
    "そして、その結果をプロットすると、次のようになります。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-32_損失のグラフ.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "図1-32 のとおり、学習を進めるに従って、損失が減っていることが分かります。  \n",
    "私たちのニューラルネットワークは、正しい方向に学習しているようです！  \n",
    "それでは、学習後のニューラルネットワークがどのような分離領域――これは決定境界（<span style=\"font-family: Consolas\">decision boundary</span>）と呼びます――を作っているのかを可視化してみましょう。  \n",
    "その結果は、図1-33 のようになります。\n",
    "\n",
    "![代替テキスト](../images/Section1/図1-33_学習後のニューラルネットワークの決定境界.png)\n",
    "\n",
    "図1-33 に示すように、学習後のニューラルネットワークは、「渦巻き」のパターンを正しく捉えていることが分かります。  \n",
    "つまり、非線形な分離領域を学習することができたのです！  \n",
    "このように、ニューラルネットワークは隠れ層を持つことで複雑な表現が可能になります。  \n",
    "さらに層を重ねることで、その表現力がより豊かになるのがディープラーニングの特徴です。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.4.4 Trainer クラス**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "前に述べたとおり、本書ではニューラルネットワークの学習を実行する機会が多くあります。  \n",
    "そこでは、先ほど行ったような学習用のコードを書く必要がありますが、毎回同じようなコードを書くのは退屈でしょう。  \n",
    "そこで本書では、学習を行うクラスを <span style=\"font-family: Consolas\">Trainer</span> クラスとして提供します。  \n",
    "中身の実装は、先ほどのソースコードとほとんど同じです。  \n",
    "一部、新しい機能を追加していますが、詳しい使い方は必要になった際に説明します。  \n",
    "  \n",
    "このクラスのイニシャライザは、ニューラルネットワーク（モデル）とオプティマイザを受け取ります。  \n",
    "具体的には、次のように使います。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(...)\n",
    "optimizer = SGD(lr=1.0)\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "そして、<span style=\"font-family: Consolas\">fit()</span> メソッドを呼んで学習を開始します。  \n",
    "この <span style=\"font-family: Consolas\">fit()</span> メソッドは表1-1\n",
    "に示す引数を持ちます。\n",
    "\n",
    "![代替テキスト](../images/Section1/表1-1_Trainerクラスのfitメソッドの引数.png)\n",
    "\n",
    "また、<span style=\"font-family: Consolas\">Trainer</span> クラスには <span style=\"font-family: Consolas\">plot()</span> というメソッドがあります。  \n",
    "これは <span style=\"font-family: Consolas\">fit()</span> メソッドで記録した損失――正確には <span style=\"font-family: Consolas\">eval_interval</span> のタイミングで評価された平均損失――をプロットします。  \n",
    "それでは、<span style=\"font-family: Consolas\">Trainer</span> クラスを使って学習を行うコードを次に示します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.optimizer import SGD\n",
    "from common.trainer import Trainer\n",
    "from dataset import spiral\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "\n",
    "x, t = spiral.load_data()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(x, t, max_epoch, batch_size, eval_interval=10)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "このコードを実行すると、前回同様にニューラルネットワークの学習が行われます。  \n",
    "以前に示した学習用のコードを <span style=\"font-family: Consolas\">Trainer</span> クラスに担わせることで、コードがスッキリしました。  \n",
    "本書ではこれ以降、<span style=\"font-family: Consolas\">Trainer</span> クラスを使って学習を行います。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "## **1.5 計算の高速化**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ニューラルネットワークの学習や推論では、多くの計算が必要になります。  \n",
    "そのため、ニューラルネットワークをいかに高速に計算するかということは重要なテーマです。  \n",
    "ここでは、ニューラルネットワークの高速化に有効な「ビット精度」と「GPU」について簡単に説明します。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_5-1.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.5.1 計算の高速化**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "NumPy の浮動小数点数は、標準で64 ビットのデータ型が使用されます。  \n",
    "（64 ビットかどうかは、読者の環境―― OS やPython/NumPy のバージョンなど――によって変わる可能性があります）。  \n",
    "実際に64 ビットの浮動小数点数が使われることは、次のコードで確かめられます。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.randn(3)\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "NumPy 配列のインスタンス変数 <span style=\"font-family: Consolas\">dtype</span> によって、データの型を見ることができます。  \n",
    "上の結果は  <span style=\"font-family: Consolas\">float64</span> と表示されていますが、これは 64ビットの浮動小数点数を表します。  \n",
    "このように NumPy では標準で 64ビットの浮動小数点数が使われます。  \n",
    "しかし、ニューラルネットワークの推論および学習は、32ビットの浮動小数点数で問題なく――認識精度をほとんど落とすことなく――行えることが知られています。  \n",
    "メモリの観点では、32ビットは 64ビットの半分になるため、常に 32ビットが好ましいと言えます。  \n",
    "また、ニューラルネットワークの計算では、データを転送する「バス帯域」がボトルネックになる場合があります。  \n",
    "その場合も、もちろんデータ型は小さいほうが望ましいです。  \n",
    "そして、計算速度の点においても、32ビット浮動小数点数のほうが高速に計算できます（浮動小数点数の計算速度は、CPU や GPU のアーキテクチャに依存します）。  \n",
    "そのため、本書では 32ビット浮動小数点数を優先して使用することにします。  \n",
    "NumPy で 32ビット浮動小数点数を使うには、次のようにデータ型を <span style=\"font-family: Consolas\">np.float32</span>や <span style=\"font-family: Consolas\">’f’<span> と指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(3).astype(np.float32)\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.random.randn(3).astype('f')\n",
    "c.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "また、ニューラルネットワークの推論に限定すれば、16ビット浮動小数点数で、認識精度をほとんど劣化させずに処理できることが分かっています。  \n",
    "ただし、NumPy には 16ビットの浮動小数点数が用意されていますが、一般的な CPU や GPU では演算自体は 32ビットで行われます。  \n",
    "そのため、16ビットの浮動小数点数に変換したところで、計算自体は 32ビットの浮動小数点数で行われてしまい、処理速度の点では恩恵を受けられません。  \n",
    "しかし、学習した重みを（外部ファイルに）保存するようなケースでは、16ビットの浮動小数点数は有効です。  \n",
    "具体的には、重みデータを 16ビット精度で保存する場合、32ビットのときの半分の容量で保存することができます。  \n",
    "そのため本書では、学習した重みを保存するときに限り、16ビットの浮動小数点数に変換することにします。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_5-2.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **1.5.2 GPU（CuPy）**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ディープラーニングの計算は、大量の積和演算によって構成されます。  \n",
    "この大量の積和演算の多くは並列計算が可能であり、それは CPU よりも GPU が得意とするところです。  \n",
    "そこで、一般的なディープラーニングのフレームワークでは、CPU に加えて GPU でも実行できるように設計されています。  \n",
    "本書では、CuPy と呼ばれる Python ライブラリをオプションとして使用します。  \n",
    "CuPy は、GPU による並列計算を行うためのライブラリです。  \n",
    "CuPy を利用するには、NVIDIA 製の GPU を備えたマシンが必要になります。  \n",
    "また、CUDA と呼ばれる  GPU 向けの汎用並列コンピューティング・プラットフォームをインストールする必要があります。  \n",
    "詳しいインストール方法は、CuPy 公式のインストール・ドキュメント を参照してください。  \n",
    "この CuPy を使えば、NVIDIA 製の GPU を使って簡単に並列計算を行うことができます。  \n",
    "さらに重要なことは、CuPy は NumPy と共通の API を持つことです。  \n",
    "簡単な使用例を示すと、次のようになります。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m6\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m x\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "x = cp.arange(6).reshape(2, 3).astype('f')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ここで示したように、CuPy は基本的に NumPy と同じ使い方ができます。  \n",
    "そして、その裏側では GPU を使って計算が行われます。  \n",
    "これが意味することは、NumPy で書かれたコードがあれば、それを “GPU 版” に簡単に変更できるということです。  \n",
    "なぜなら、そのとき私たちが行うことは、（基本的には）<span style=\"font-family: Consolas\">numpy</span> を <span style=\"font-family: Consolas\">cupy</span> に置き換えるだけだからです！\n",
    "\n",
    "![代替テキスト](../images/Section1/Caution_1_5-1.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "繰り返しになりますが、本書では実装の分かりやすさを優先し、CPU による実装を基本とします。  \n",
    "ただし計算に多くの時間がかかるコードに関しては、オプションとして CuPy を使った実装を提供します。  \n",
    "ただし、CuPy を使う場合においても、読者が CuPy を使うことを意識する必要はないようにコード上で配慮しています。  \n",
    "\n",
    "このコードの実行には CPU で数時間かかりますが、GPU を使えば数十分程度で完了します。  \n",
    "そして、本書が提供するコードでは、上のソースコードを１行修正するだけで GPU モードで実行できます。  \n",
    "具体的には「# config.GPU = True」のコメントアウトを外せば、NumPy の代わりに CuPy が使われるようになります。  \n",
    "それによって、GPU 上で実行され、高速に学習が行われます。  \n",
    "GPU をお持ちの方は、ぜひ利用してみましょう。\n",
    "\n",
    "![代替テキスト](../images/Section1/Hint1_5-3.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "## **1.6 まとめ**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "本章では、ニューラルネットワークの基本を復習しました。  \n",
    "まずはベクトルや行列などの数学の復習からスタートし、Python（特に NumPy）の基本的な使い方の確認を行いました。  \n",
    "そして、ニューラルネットワークの仕組みを見てきました。  \n",
    "特に、計算グラフの基本パーツ（加算ノードや乗算ノードなど）をいくつか取り上げ、その順伝播と逆伝播を説明しました。  \n",
    "また、ニューラルネットワークの実装も行いました。私たちはモジュール性を考慮し、ニューラルネットワークの基本パーツをレイヤとして実装しました。  \n",
    "レイヤの実装では、クラスのメソッドとして <span style=\"font-family: Consolas\">forward()</span> と <span style=\"font-family: Consolas\">backward()</span> を持つこと、インスタンス変数として <span style=\"font-family: Consolas\">params</span> と <span style=\"font-family: Consolas\">grads</span> を持つことを本書の「実装ルール」としました。  \n",
    "これによって、ニューラルネットワークの実装が見通しの良いものとなりました。  \n",
    "最後に本章では、人工的な「渦巻きデータセット」に対して、隠れ層がひとつあるニューラルネットワークで学習を行いました。  \n",
    "そして、そのモデルが正しく学習できることを確認しました。  \n",
    "これでニューラルネットワークの復習は終わりです。  \n",
    "これからニューラルネットワークという頼もしい武器を手に、自然言語処理の世界へと乗り込みます。それでは先に進みましょう！\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
