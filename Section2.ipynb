{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "# **2章　自然言語と単語の分散表現**\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "これからいよいよ自然言語処理の世界へ足を踏み入れます。  \n",
    "自然言語処理が扱う分野は多岐にわたりますが、  \n",
    "その本質的問題はコンピュータに私たちの言葉を理解させることにあります。  \n",
    "\n",
    "本章では、コンピュータに言葉を理解させるとはどういうことか、  \n",
    "そしてどのようなアプローチが存在するのか、ということを中心に考察を進めます。  \n",
    "\n",
    "特に古典的な手法 ――ディープラーニング登場以前の手法―― について詳しく見ていきます。  \n",
    "そして次章から、ディープラーニングをベースとした手法へと進みます。  \n",
    "\n",
    "また、本章ではテキストを Python で扱う練習も行います。  \n",
    "テキストを単語に分割する処理や、単語を単語 ID に変換する処理などを実装します。  \n",
    "\n",
    "そして本章で実装する関数は、次章以降でも利用します。  \n",
    "つまり本章は、これから先のテキスト処理の下準備も兼ねています。  \n",
    "\n",
    "それでは、自然言語処理の世界へと進みましょう！\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "## **2.1 自然言語処理とは**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "**自然言語処理**（Natural Language Processing：NLP）とは、文字どおり解釈すれば、「自然言語を処理する分野」。  \n",
    "分かりやすく言うと、「私たちの言葉をコンピュータに理解させるための技術（分野）」です。  \n",
    "つまり、自然言語処理の目標とすることは、人の話す言葉をコンピュータに理解させ、  \n",
    "私たちにとって役に立つことをコンピュータに行わせることにあります。  \n",
    "\n",
    "コンピュータが理解できる「プログラミング言語」や「マークアップ言語」は、  \n",
    "コードの意味が一意に解釈できるように文法が定義されています。  \n",
    "これは言ってみれば、**\"固い言語\"** と言えます。  \n",
    "\n",
    "一方、英語や日本語などの自然言語は、**\"柔らかい言語\"** です。  \n",
    "「柔らかい」というのは、同じ意味の文章にさまざまな表現があったり、文章に曖昧さがあったりと、意味や形が柔軟に変わることを意味します。  \n",
    "また、時代とともに新しい言葉や新しい意味が生まれます（または、廃れます）。これも自然言語の \"柔らかさ\" の表れです。\n",
    "\n",
    "頭の固いコンピュータに自然言語を理解させるという難問をクリアできれば、人にとって役に立つことをコンピュータに行わせることができます。  \n",
    "たとえば、検索エンジンや機械翻訳、質問応答システムやIME（かな漢字変換）、文章の自動要約や感情分析など、  \n",
    "私たちの身の回りではすでに自然言語処理の技術が数多く使われています。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "![代替テキスト](./kazuki.a_images/Section2/Hint/Hint2-1_1.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **2.1.1 単語の意味**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "私たちの言葉の意味は「**単語**」によって構成されます。  \n",
    "**単語**は、言ってみれば、**意味の最小単位**です。  \n",
    "そのため、自然言語をコンピュータに理解させるためには、**「単語の意味」を理解させる**ことが重要であると言えそうです。\n",
    "\n",
    "本章のテーマは、コンピュータに「単語の意味」を理解させることです。  \n",
    "より正確に言うと、「単語の意味」をうまく捉えた表現方法について考えます。  \n",
    "具体的には、本章と次章で次の３つの手法を見ていきます。\n",
    "\n",
    "- シソーラスによる手法【本章】\n",
    "- カウントベースの手法【本章】\n",
    "- 推論ベースの手法（word2vec）【次章】\n",
    "\n",
    "最初に、人の手によって作られた**シソーラス**（類語辞書）を利用する方法について簡単に見ていきます。  \n",
    "続いて、統計情報から単語を表現する手法 ――ここでは「**カウントベースの手法**」と呼ぶことにします―― について説明します。  \n",
    "ここまでが本章で学ぶ内容です。  \n",
    "\n",
    "そして次章では、ニューラルネットワークによる「**推論ベース**」の手法を扱います。  \n",
    "なお、本章の構成は、スタンフォード大学の授業「CS224d: Deep Learning for Natural Language Processing」[10] を参考にしています。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "## **2.2 シソーラス**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "「単語の意味」を表すためには、人の手によって単語の意味を定義することが考えられます。  \n",
    "\n",
    "そのひとつの方法として、『広辞苑』などの辞書のように、一つひとつの単語に対してその単語の意味を説明していくことが考えられます。  \n",
    "たとえば、「自動車」という単語を辞書で引くと、「車輪を取り付けてそれによって進むようになっている乗り物や運搬具……」のような説明が続きます。  \n",
    "\n",
    "そのように単語を定義すれば、コンピュータも単語の意味を理解できるかもしれません。  \n",
    "自然言語処理の歴史を振り返ってみると、単語の意味を人手で定義するという試みは数多く行われてきました。  \n",
    "ただし、『広辞苑』のように人が使う一般的な辞書ではなく、**シソーラス**（thesaurus）と呼ばれるタイプの辞書が多く使われてきました。\n",
    "\n",
    "**シソーラス** とは（基本的には）類語辞書であり、「同じ意味の単語（同義語）」や「意味の似た単語（類義語）」が同じグループに分類されています。  \n",
    "たとえば、`car` の同義語には、`automobile` や `motorcar` などが存在することが、シソーラスを使えば分かります（図2-1 参照）。\n",
    "\n",
    "![代替テキスト](./kazuki.a_images/Section2/Figure/図2-1_同義語の例.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "また、自然言語処理において利用されるシソーラスでは、  \n",
    "単語の間で「上位と下位」「全体と部分」など、より細かい関連性が定義されている場合があります。  \n",
    "\n",
    "具体的には、図2-2 のように、グラフ構造によって各単語の関係性が定義されているケースです。\n",
    "![代替テキスト](./kazuki.a_images/Section2/Figure/図2-2_単語ネットワーク.png)\n",
    "\n",
    "図2-2 では、「car」という単語の上位概念として、「motor vehicle（動力車）」という単語が位置しています。  \n",
    "また、「car」の下位の概念に、「SUV」「compact」「hatch-back」など、より具体的な車種があることが示されています。  \n",
    "\n",
    "このようにすべての単語に対して、類義語の集合を作り、また、それぞれの単語の関係をグラフで表現することで、単語間のつながりを定義できます。  \n",
    "私たちはこの \"単語ネットワーク\" を利用することで、コンピュータに単語間の関連性を教えることができます。\n",
    "\n",
    "これは、コンピュータに単語の意味を（間接的にであれ）授けることができたと言えそうです。  \n",
    "そして、その知識を利用すれば、私たちにとって有用なことをコンピュータに行わせることができるでしょう。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "![代替テキスト](./kazuki.a_images/Section2/Hint/Hint2-2_1.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **2.2.1 WordNet**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "自然言語処理の分野において、最も有名なシソーラスは **WordNet** です。  \n",
    "\n",
    "WordNet はプリンストン大学で1985年に開発がスタートした伝統あるシソーラスで、これまでに多くの研究で利用されてきました。  \n",
    "また、さまざまな自然言語処理のアプリケーションにおいても大いに活躍しています。  \n",
    "\n",
    "WordNet を使えば、類義語を取得したり、“単語ネットワーク” を利用したりすることができます。  \n",
    "また、単語ネットワークを使って単語間の類似度を算出することも可能です。  \n",
    "\n",
    "ここでは、WordNet について詳細な説明は行いません。  \n",
    "WordNet を使ったPython の実装について興味のある方は、「付録B WordNet を動かす」を参照してください。  \n",
    "付録B では、WordNet をインストールして（正確には NLTK というモジュールをインストールして）、いくつかの簡単な実験を行います。\n",
    "\n",
    "![代替テキスト](./kazuki.a_images/Section2/Hint/Hint2-2_2.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **2.2.1 シソーラスの問題点**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "WordNet のようなシソーラスでは、多くの単語に対して同義語や階層構造などの関係性が定義されています。  \n",
    "そして、それらの知識を利用すれば、「単語の意味」というものを（間接的にであれ）コンピュータに授けることができそうです。  \n",
    "しかし、それらの人の手によるラベル付けには次の大きな欠点が存在します。  \n",
    "<br>\n",
    "\n",
    "**時代の変化への対応が困難**  \n",
    "私たちの使う言葉は生きています。時とともに新しい言葉が生まれ、古い言葉はいつの日か忘れ去られます。  \n",
    "たとえば、「クラウド・ファンディング（crowdfunding）」という言葉は、最近使われ始めた新しい造語です。  \n",
    "また、時代によって言葉の意味が変化するケースもあります。  \n",
    "たとえば、「heavy」には、「（ものごとが）深刻である」という意味がありますが、昔はそのような使い方はされませんでした。  \n",
    "このような単語の変化に対応するには、シソーラスを人手によって絶えず更新しなければなりません。  \n",
    "\n",
    "**人の作業コストが高い**  \n",
    "シソーラスを作るには、大変な人的コストが発生します。  \n",
    "英語を例に挙げると、現存する英単語の総数は 1,000 万語を超えると言われています。  \n",
    "そのため、理想的にはそのような膨大な単語に対して、単語の関連付けを行う必要があるのです。  \n",
    "ちなみに WordNet では、20 万語を超える単語が登録されているそうです。\n",
    "\n",
    "**単語の細かなニュアンスを表現できない**  \n",
    "シソーラスでは、類義語として似たような単語をグループ化します。  \n",
    "しかし実際には、似たような単語であっても、それぞれにニュアンスは異なるものです。  \n",
    "たとえば、「ヴィンテージ」という単語と「レトロ」という単語は同じような意味を表しますが、その使われ方は異なります。  \n",
    "シソーラスでは、このような微妙なニュアンスの差異を表すことができません（もし人手で表すとすれば、それは相当に困難なものになるでしょう）。  \n",
    "<br>\n",
    "\n",
    "このように、シソーラスを使う手法には大きな問題があります。   \n",
    "このような問題を避けるために「カウントベースの手法」、ニューラルネットワークを使った「推論ベースの手法」へと進みます。  \n",
    "それら２つの手法では、大量のテキストデータから自動的に「単語の意味」を抽出します。  \n",
    "そのため、人の手作業によって単語を関連付けるというような重労働から解放されるのです！\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\">\n",
    "\n",
    "![代替テキスト](./kazuki.a_images/Section2/Hint/Hint2-2_3.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "## **2.3 カウントベースの手法**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "これからカウントベースの手法に進むにあたって、私たちは **コーパス**（corpus）を利用します。  \n",
    "**コーパス** とは、簡単に言ってしまえば、大量のテキストデータです。  \n",
    "ただし、やみくもに集められたテキストデータではなく、  \n",
    "自然言語処理の研究やアプリケーションのために目的をもって収集されたテキストデータを、一般的には「コーパス」と呼びます。\n",
    "\n",
    "結局のところ、コーパスとはテキストデータにすぎません。  \n",
    "しかし、そこに含まれる文章は人の手によって書かれたものです。  \n",
    "これは別の見方をすれば、コーパスには自然言語に対する人の \"知識\" がふんだんに含まれているということです。  \n",
    "文章の書き方、単語の選び方、そして、単語の意味――そのような人の自然言語に対する知識が、コーパスには含まれています。  \n",
    "カウントベースの手法の目標は、そのような人の知識が詰まったコーパスから、自動的に、そして効率良く、そのエッセンスを抽出することにあります。\n",
    "\n",
    "![代替テキスト](./kazuki.a_images/Section2/Caution/Caution2-3_1.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **2.3.1 Python によるコーパスの下準備**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "自然言語処理の分野で用いられるコーパスには、さまざまなものが存在します。  \n",
    "有名なもので言えば、Wikipedia や Google News などのテキストデータが挙げられます。  \n",
    "また、シェイクスピアや夏目漱石など、偉大な作家の作品群もコーパスとして利用されます。  \n",
    "本章では、まず初めに１文からなる単純なテキストをコーパスとして利用します。  \n",
    "その後で、より実用的なコーパスを扱います。  \n",
    "それでは Python の対話モードを使って、とても小さなテキストデータ（コーパス）に前処理を行います。  \n",
    "ここで言う前処理とは、テキストデータを単語に分割し、その分割した単語を単語 ID のリストへと変換することです。  \n",
    "では、ひとつずつ確認しながら、ステップ・バイ・ステップで組み立てていきましょう。  \n",
    "まずは、今回コーパスとして利用するサンプルの文章からです。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'You say goodbye and I say hellow.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ここではひとつの文からなるテキストをコーパスとして利用します。  \n",
    "本来であれば、このテキスト（<span style=\"font-family: Consolas\">text</span>）には何千、何万を超える文が（連結されて）含まれます。  \n",
    "しかし、ここでは簡易性を優先して、この小さなテキストデータに対して前処理を行っていきます。  \n",
    "それでは、上の <span style=\"font-family: Consolas\">text</span> を単語単位に分割しましょう。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you say goodbye and i say hellow .'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text = text.replace('.', ' .')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hellow', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text.split(' ')\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ここでは最初に、すべての文字を小文字に変換するために <span style=\"font-family: Consolas\">lower()</span> メソッドを使います。  \n",
    "これは、文頭の単語でも共通の単語として扱えるようにするための処置です。  \n",
    "そして、<span style=\"font-family: Consolas\">split(' ')</span> によって、スペースを「区切り文字」として分割します。  \n",
    "ただしここでは文末のピリオド (<span style=\"font-family: Consolas\">.</span>) を考慮して、ピリオドの前にスペースを挿入してから分割を行っています。\n",
    "\n",
    "![代替テキスト](./kazuki.a_images/Section2/Caution/Caution2-3_2.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "これで、元の文章を単語のリストとして利用できるようになりました。  \n",
    "単語が分割されたことで扱いやすくなりましたが、単語をテキストのまま操作するのは何かと不便です。  \n",
    "そこで、単語に ID を振って、ID のリストとして利用できるように、さらに手を加えます。  \n",
    "その下準備として、単語の ID と単語の対応表を Python のディクショナリで作成します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "単語 ID → 単語 の変換を <span style=\"font-family: Consolas\">id_to_word</span> という変数が担当し（キー: 単語ID、値: 単語）、  \n",
    "単語 → 単語 ID の変換を <span style=\"font-family: Consolas\">word_to_id</span> が担当します。  \n",
    "\n",
    "ここでは単語単位に分割された <span style=\"font-family: Consolas\">words</span> の各要素を先頭からひとつずつ見ていき、  \n",
    "単語が <span style=\"font-family: Consolas\">word_to_id</span> に存在しない場合は、<span style=\"font-family: Consolas\">word_to_id</span> と <span style=\"font-family: Consolas\">id_to_word</span> に新しい ID と単語をそれぞれ追加します。  \n",
    "\n",
    "なお、ディクショナリの長さを新しい単語 ID として設定するため、単語 ID は、0、1、2……と増えていくことになります。  \n",
    "これで、単語 ID と単語の対応表を作ることができました。それでは、実際に中身を見てみましょう。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hellow', 6: '.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hellow': 5, '.': 6}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "それでは最後に、「単語のリスト」を「単語ID のリスト」に変換しましょう。  \n",
    "ここでは、Python の内包表記を使って、単語リストから単語ID リストへ変換します。  \n",
    "そして、それをNumPy 配列に変換します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 1, 5, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![代替テキスト](./kazuki.a_images/Section2/Hint/Hint2-3_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "これで、コーパスを利用する下準備が整いました。  \n",
    "それでは以上の処理を <span style=\"font-family: Consolas\">preprocess()</span> という名前の関数として、まとめて実装することにします。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "この関数を使えば、コーパスの前処理は次のように行うことができます。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "これでコーパスの前処理は終わりです。  \n",
    "ここで準備した <span style=\"font-family: Consolas\">corpus</span>、<span style=\"font-family: Consolas\">word_to_id</span>、<span style=\"font-family: Consolas\">id_to_word</span> という３つの変数名は、これから先も本書の多くの場所で登場します。  \n",
    "<span style=\"font-family: Consolas\">corpus</span> は単語 ID のリスト、<span style=\"font-family: Consolas\">word_to_id</span> は単語から単語 ID へのディクショナリ、<span style=\"font-family: Consolas\">id_to_word</span> は単語 ID から単語へのディクショナリを表します。  \n",
    "以上でコーパスを扱う準備が整いました。  \n",
    "\n",
    "私たちのこれからの目標は、コーパスを使って「単語の意味」を抽出することです。  \n",
    "そのために本節では「カウントベースの手法」を見ていきます。  \n",
    "この手法によって、私たちは単語をベクトルで表すことができるようになるのです。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **2.3.2 単語の分散表現**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "突然ですが、世の中にはさまざまな「色」が存在します。  \n",
    "たとえば、「コバルトブルー」や「シンクレッド」といったように、色には固有の名前が付けられています。  \n",
    "その一方で、色は RGB（Red/Green/Blue）の３成分がどれだけ存在するかといった方法でも表すことができます。  \n",
    "\n",
    "前者が色の数だけ異なる名前を命名する一方で、後者は、色を**３次元のベクトルとして表現**します。  \n",
    "ここで注目したい点は、RGB のようなベクトル表現のほうが、正確に色を指定できるということです。  \n",
    "さらに、３つの成分というコンパクトな表現が可能であり、（多くの場合）色のイメージがつきやすいという利点もあります。  \n",
    "\n",
    "たとえば、「こきあけ深緋」という色がどのような色か分からなくても、(R, G, B) = (201, 23, 30) と言われれば、それは赤系の色だということが分かります。  \n",
    "また、色どうしの関連性 ――似た色かどうかなど―― もベクトル表現のほうが容易に判断しやすく、定量化も簡単に行えます。  \n",
    "\n",
    "それでは、ここで話した「色」のベクトル表現のようなことを、「単語」でも行えないでしょうか？  \n",
    "より正確に言うならば、コンパクトで理にかなったベクトル表現を、「単語」というフィールドにおいても構築できないでしょうか？  \n",
    "\n",
    "これから私たちが目指すべき場所は、「単語の意味」を的確に捉えた**ベクトル表現**です。  \n",
    "これは自然言語処理の分野では、単語の**分散表現**と呼ばれます。\n",
    "\n",
    "![代替テキスト](./kazuki.a_images/Section2/Hint/Hint2-3_2.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **2.3.3 分布仮説**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "自然言語処理の歴史において、単語をベクトルで表す研究は数多く行われてきました。  \n",
    "そのような研究を見ていくと、重要な手法のほとんどすべてが、あるひとつのシンプルなアイデアに基づいていることが分かります。  \n",
    "\n",
    "そのアイデアとは、「**単語の意味は、周囲の単語によって形成される**」というものです。  \n",
    "これは、**分布仮説**（distributional hypothesis）と呼ばれるもので、単語をベクトルで表す最近の研究の多くが、この仮説に基づいています。\n",
    "\n",
    "分布仮説が言わんとすることは、とてもシンプルです。  \n",
    "単語自体には意味がなく、その単語の「コンテキスト（文脈）」によって、単語の意味が形成されると言うのです。  \n",
    "確かに、意味的に同じ単語は、同じような文脈で多く出現します。  \n",
    "たとえば、「I drink beer.」「We drink wine.」のように drink の近くには飲み物が表れやすいでしょう。  \n",
    "また、「I guzzle beer.」「We guzzle wine.」のような文章があれば、guzzle という単語が drink と同じような文脈で使われるということが分かります。  \n",
    "そして、guzzle と drink が近い意味の単語だということも導けます（ちなみに、guzzle とは「がぶがぶ飲む」という意味の単語です）。\n",
    "\n",
    "さて、これから先、「コンテキスト」という言葉を多く使います。  \n",
    "本章で「コンテキスト」と言うとき、それは（注目する単語に対して）その周囲に存在する単語を指します。  \n",
    "たとえば、図2-3 の例では、左右の２単語が「コンテキスト」に相当します。\n",
    "\n",
    "![代替テキスト](./kazuki.a_images/Section2/Figure/図2-3_コンテキスト.png)\n",
    "\n",
    "図2-3 に示すとおり、「コンテキスト」とは、ある中央の単語に対して、その周囲にある単語を指します。  \n",
    "またここでは、コンテキストのサイズ――つまり、周囲の単語をどれだけ含めるかということ――を、「ウィンドウサイズ（window size）」という言葉で表すことにします。  \n",
    "ウィンドウサイズが１の場合は左右の１単語、ウィンドウサイズが２の場合は左右の２単語、といったようになります。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "![代替テキスト](./kazuki.a_images/Section2/Caution/Caution2-3_3.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red;\">\n",
    "\n",
    "### **2.3.4 共起行列**\n",
    "</div>\n",
    "\n",
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "それでは、分布仮説に基づいて、単語をベクトルで表す方法を考えましょう。  \n",
    "そのための素直な方法は、周囲の単語を \"カウント\" することです。  \n",
    "\n",
    "具体的に言うと、ある単語に着目した場合、その周囲にどのような単語がどれだけ現れるのかをカウントし、集計するのです。  \n",
    "ここではこれを「カウントベースの手法」と呼ぶことにします。  \n",
    "なお、これは文献によっては「統計的手法」と呼ばれる場合もあります。  \n",
    "\n",
    "それでは、カウントベースの手法を見ていきましょう。  \n",
    "ここでは、「2.3.1 Python によるコーパスの下準備」のコーパスと <span style=\"font-family: Consolas\">preprocess()</span> 関数を使用して、再度下準備を行うところからスタートします。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "上の結果より、ここでは語彙数が全部で７個あることが分かります。  \n",
    "それでは、それぞれの単語について、そのコンテキストに含まれる単語の頻度を数えていきます。  \n",
    "ここでは、ウィンドウサイズを 1 として、まずは単語ID が 0 の「you」からスタートします。\n",
    "\n",
    "![代替テキスト](./kazuki.a_images/Section2/Figure/図2-4_youのコンテキストカウント.png)\n",
    "\n",
    "図2-4 を見れば一目瞭然ですが、単語「you」のコンテキストには「say」という単語がひとつだけ存在することが分かります。  \n",
    "\n",
    "これをテーブルで表せば、図2-5 のようになります。\n",
    "\n",
    "![代替テキスト](./kazuki.a_images/Section2/Figure/図2-5_youのコンテキストカウントテーブル.png)\n",
    "\n",
    "図2-5 は、単語「you」に対して、コンテキストとして共起する単語の頻度を表しています。  \n",
    "そしてこれは同時に「you」という単語が <span style=\"font-family: Consolas\">[0, 1, 0, 0, 0, 0, 0]</span> というベクトルで表現できるということも意味します。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "それでは、続いて単語 ID が 1 の「say」について、同じ作業を行います。  \n",
    "\n",
    "その結果は、図2-6 のようになります。\n",
    "\n",
    "![代替テキスト](./kazuki.a_images/Section2/Figure/図2-6_sayのコンテキストカウントテーブル.png)\n",
    "\n",
    "上の結果から、「say」という単語は <span style=\"font-family: Consolas\">[1, 0, 1, 0, 1, 1, 0]</span> というベクトルで表現できることが分かります。  \n",
    "以上の作業を、すべての単語について行います。その結果は、図2-7 のようになります。\n",
    "\n",
    "![代替テキスト](./kazuki.a_images/Section2/Figure/図2-7_各単語のコンテキストカウントテーブル.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "図2-7 は、すべての単語に対して、共起する単語をテーブルにまとめたものです。  \n",
    "このテーブルの各行が、該当する単語のベクトルに対応します。  \n",
    "なお、図2-7 のテーブルは行列の形をしているため、**共起行列**（co-occurrence matrix）と呼ばれます。  \n",
    "それでは、上の共起行列を実際に作ってみましょう。  \n",
    "ここでは、図2-7 の結果をそのまま手で打ち込むことにします。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0],\n",
    "], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "これで共起行列ができました。この共起行列を使えば、各単語のベクトルは次のようにして得ることができます。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語ID が 0 のベクトル: [0 1 0 0 0 0 0]\n",
      "単語ID が 4 のベクトル: [0 1 0 1 0 0 0]\n",
      "'goodbye' のベクトル : [0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"単語ID が 0 のベクトル:\", C[0])\n",
    "print(\"単語ID が 4 のベクトル:\", C[4])\n",
    "print(\"'goodbye' のベクトル :\", C[word_to_id['goodbye']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ここで示したように、私たちは共起行列によって単語をベクトルで表すことができました。  \n",
    "ここでは手動で共起行列を作りましたが、もちろんこれは自動化できます。  \n",
    "それでは、コーパスから共起行列を作る関数を実装しましょう。  \n",
    "\n",
    "ここでは、<span style=\"font-family: Consolas\">create_co_matrix(corpus, vocab_size, window_size=1)</span> という関数名で実装します。  \n",
    "引数の <span style=\"font-family: Consolas\">corpus</span> は単語 ID のリスト、<span style=\"font-family: Consolas\">vocab_size</span> は語彙数、<span style=\"font-family: Consolas\">window_size</span> はウィンドウサイズを表します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: black; font-size: 20px\">\n",
    "\n",
    "ここでは、まず初めに <span style=\"font-family: Consolas\">co_matrix</span> を要素が 0 の２次元配列で初期化します。  \n",
    "後は、コーパス中の各単語すべてに対して、そのウィンドウに含まれる単語をカウントしていきます。  \n",
    "このとき、コーパスの左端と右端において、はみ出していないかのチェックを行います。  \n",
    "これでコーパスがどれだけ大きくなったとしても、自動で共起行列を作ることができます。  \n",
    "これ以降、私たちはこの関数を使ってコーパスの共起行列を作成します。\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
